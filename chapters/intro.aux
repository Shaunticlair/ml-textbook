\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\zref@newlabel[2]{}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction - Explanatory Notes}{13}{chapter.1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{intro}{{1}{13}{Introduction - Explanatory Notes}{chapter.1}{}}
\newlabel{intro@cref}{{[chapter][1][]1}{13}}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Machine Learning: What, why, and how?}{14}{section.1.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.1}What is machine learning?}{14}{subsection.1.1.1}}
\@writefile{loe}{\addvspace {10\p@ }}
\@writefile{loe}{\contentsline {concept}{\numberline {5}Concept}{14}{concept.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.2}Why do Machine Learning: The Benefits}{14}{subsection.1.1.2}}
\@writefile{tdo}{\contentsline {todo}{\linespread  {0.9}\selectfont  {Based on speed, time to develop, "robustness", etc.}\par }{14}{section*.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.3}The role of humans}{14}{subsection.1.1.3}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}The Problem and Solution of Machine Learning}{15}{section.1.2}}
\@writefile{tdo}{\contentsline {todo}{\linespread  {0.9}\selectfont  {This breakdown is different from the one above!}\par }{15}{section*.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.1}What's the plan?}{16}{subsection.1.2.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.2}The Problem}{16}{subsection.1.2.2}}
\@writefile{tdo}{\contentsline {todo}{\linespread  {0.9}\selectfont  {In that way, our machine is just a \textbf  {function}.}\par }{16}{section*.6}}
\@writefile{tdo}{\contentsline {todo}{\linespread  {0.9}\selectfont  {We often use these assumptions to come up with solutions: if they aren't true, your approach may fail!}{\@@par }}{16}{section*.7}}
\@writefile{tdo}{\contentsline {todo}{\linespread  {0.9}\selectfont  {"\textbf  {Which group} of problems does ours \textbf  {fit into?}" }{\@@par }}{16}{section*.8}}
\@writefile{tdo}{\contentsline {todo}{\linespread  {0.9}\selectfont  {In order to answer a question, you need to know what you're being asked!}{\@@par }}{16}{section*.9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.3}Solution Setup: What is a model?}{17}{subsection.1.2.3}}
\@writefile{tdo}{\contentsline {todo}{\linespread  {0.9}\selectfont  {We do this sometimes because we don't \textit  {know} the true model, and sometimes because simulating the true model is too expensive and time-consuming.}\par }{17}{section*.10}}
\@writefile{loe}{\contentsline {definition}{\numberline {6}Definition}{17}{definition.6}}
\@writefile{tdo}{\contentsline {todo}{\linespread  {0.9}\selectfont  {Again, we emphasize that a model doesn't have to be structured to match reality - but if we know the true model, this can help.}\par }{17}{section*.11}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.4}The Solution}{18}{subsection.1.2.4}}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Assumptions}{18}{section.1.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.1}An assumption about data}{18}{subsection.1.3.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.2}Is our data representative?}{19}{subsection.1.3.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.3}How do we compare data?}{19}{subsection.1.3.3}}
\@writefile{tdo}{\contentsline {todo}{\linespread  {0.9}\selectfont  {This is how our system "behaves", in a way.}\par }{19}{section*.12}}
\@writefile{loe}{\contentsline {definition}{\numberline {7}Definition}{19}{definition.7}}
\@writefile{tdo}{\contentsline {todo}{\linespread  {0.9}\selectfont  {Why is it called a distribution? Well, we're taking the \textbf  {odds}, and spreading them out (or \textbf  {distributing} them) over multiple different outcomes!}\par }{19}{section*.13}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.4}Identically Distributed Data}{20}{subsection.1.3.4}}
\@writefile{tdo}{\contentsline {todo}{\linespread  {0.9}\selectfont  {We want to focus on one problem at a time - one distribution.}\par }{20}{section*.14}}
\@writefile{loe}{\contentsline {definition}{\numberline {8}Definition}{20}{definition.8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.5}Independence (Review)}{20}{subsection.1.3.5}}
\@writefile{loe}{\contentsline {definition}{\numberline {9}Definition}{20}{definition.9}}
\@writefile{tdo}{\contentsline {todo}{\linespread  {0.9}\selectfont  {This definition is a bit informal: the proper definition is to say that, for two events A and B, $P(A)P(B) = P(A \text  { and } B)$}\par }{20}{section*.15}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.6}Independent and Identically Distributed}{21}{subsection.1.3.6}}
\@writefile{loe}{\contentsline {definition}{\numberline {10}Definition}{21}{definition.10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.7}Estimation and Generalization}{21}{subsection.1.3.7}}
\@writefile{loe}{\contentsline {definition}{\numberline {11}Definition}{21}{definition.11}}
\@writefile{tdo}{\contentsline {todo}{\linespread  {0.9}\selectfont  {Just like how background \textbf  {noise} can make it harder to listen to a phone call!}\par }{21}{section*.16}}
\@writefile{loe}{\contentsline {definition}{\numberline {12}Definition}{22}{definition.12}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.8}Other Assumptions}{22}{subsection.1.3.8}}
\@writefile{tdo}{\contentsline {todo}{\linespread  {0.9}\selectfont  {Imagine if you were supposed to write an essay, but could only answer with real numbers between 0 and 1 - this is what we want to avoid.}{\@@par }}{22}{section*.17}}
\@writefile{tdo}{\contentsline {todo}{\linespread  {0.9}\selectfont  {If you don't know what this is, don't worry! We come back to it later.}{\@@par }}{22}{section*.18}}
\@writefile{toc}{\contentsline {section}{\numberline {1.4}Problem Class}{22}{section.1.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.1}Supervised vs. Unsupervised}{22}{subsection.1.4.1}}
\@writefile{tdo}{\contentsline {todo}{\linespread  {0.9}\selectfont  {In a way, we're "supervising" it by giving it the right answer: we're guiding it and making sure it does what we want it to!}\par }{22}{section*.19}}
\@writefile{tdo}{\contentsline {todo}{\linespread  {0.9}\selectfont  {You don't need to know the species "cow" and "pig" to figure out that they're different from each other! }{\@@par }}{23}{section*.20}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.2}How do we store our data?}{23}{subsection.1.4.2}}
\@writefile{tdo}{\contentsline {todo}{\linespread  {0.9}\selectfont  {Being consistent makes it easier to develop the techniques we need!}\par }{23}{section*.21}}
\@writefile{loe}{\contentsline {notation}{\numberline {13}Notation}{23}{notation.13}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.3}Supervised Learning}{24}{subsection.1.4.3}}
\@writefile{tdo}{\contentsline {todo}{\linespread  {0.9}\selectfont  {Notice that "where they live" isn't usually represented as a number: we often have to convert certain data types.}{\@@par }}{24}{section*.22}}
\@writefile{tdo}{\contentsline {todo}{\linespread  {0.9}\selectfont  {As before, a "class" is just another word for a group of related things.}{\@@par }}{24}{section*.23}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.4}Unsupervised Learning}{24}{subsection.1.4.4}}
\@writefile{tdo}{\contentsline {todo}{\linespread  {0.9}\selectfont  {This data point is our outcome!}{\@@par }}{24}{section*.24}}
\@writefile{tdo}{\contentsline {todo}{\linespread  {0.9}\selectfont  {Since we often automate this process, real examples might not be so simple!}{\@@par }}{24}{section*.25}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.5}Other Types of Learning}{25}{subsection.1.4.5}}
\@writefile{tdo}{\contentsline {todo}{\linespread  {0.9}\selectfont  {We represent the current environment with something called a \textbf  {state}.}{\@@par }}{25}{section*.26}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.6}Types of Learning not covered in this class (Optional)}{25}{subsection.1.4.6}}
\@writefile{toc}{\contentsline {section}{\numberline {1.5}Evaluation Criteria}{26}{section.1.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.5.1}What is a loss function?}{26}{subsection.1.5.1}}
\@writefile{loe}{\contentsline {definition}{\numberline {14}Definition}{26}{definition.14}}
\@writefile{loe}{\contentsline {notation}{\numberline {15}Notation}{26}{notation.15}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.5.2}Examples of Loss Functions}{26}{subsection.1.5.2}}
\@writefile{tdo}{\contentsline {todo}{\linespread  {0.9}\selectfont  {This matches our earlier example of "number of questions wrong on a test".}{\@@par }}{26}{section*.27}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.5.3}How to use loss}{27}{subsection.1.5.3}}
\@writefile{tdo}{\contentsline {todo}{\linespread  {0.9}\selectfont  {We could the "worst-case" loss, or the average loss, etc...}\par }{27}{section*.28}}
\@writefile{tdo}{\contentsline {todo}{\linespread  {0.9}\selectfont  {This is called the \textbf  {law of large numbers}: if you have a large number of trials, the average value should be close to the expected value.}\par }{27}{section*.29}}
\@writefile{loe}{\contentsline {concept}{\numberline {16}Concept}{27}{concept.16}}
\@writefile{toc}{\contentsline {section}{\numberline {1.6}Model Type}{27}{section.1.6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.6.1}No Model}{27}{subsection.1.6.1}}
\@writefile{tdo}{\contentsline {todo}{\linespread  {0.9}\selectfont  {When we say "similar", there are multiple ways to interpret this, but often we use distance in $\mathbb  R^d$ space.}\par }{27}{section*.30}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.6.2}Models using Parameters}{28}{subsection.1.6.2}}
\@writefile{loe}{\contentsline {definition}{\numberline {17}Definition}{28}{definition.17}}
\@writefile{tdo}{\contentsline {todo}{\linespread  {0.9}\selectfont  {"Almost" because, if $A=B=0$ for both cases, they're both the constant function $f(x)=C$.}\par }{28}{section*.31}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.6.3}Prediction Rule}{28}{subsection.1.6.3}}
\@writefile{loe}{\contentsline {definition}{\numberline {18}Definition}{29}{definition.18}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.6.4}Hypothesis Notation}{29}{subsection.1.6.4}}
\@writefile{loe}{\contentsline {notation}{\numberline {19}Notation}{29}{notation.19}}
\@writefile{tdo}{\contentsline {todo}{\linespread  {0.9}\selectfont  {Often, a vector is represented by bolding a variable ($\mathbf  {x}$, or putting an arrow over it ($\mathaccentV {vec}17E{x}$). Since we work with vectors so often in this class, we will omit this notation.}\par }{29}{section*.32}}
\@writefile{loe}{\contentsline {notation}{\numberline {20}Notation}{29}{notation.20}}
\@writefile{loe}{\contentsline {notation}{\numberline {21}Notation}{29}{notation.21}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.6.5}Fitting}{29}{subsection.1.6.5}}
\@writefile{tdo}{\contentsline {todo}{\linespread  {0.9}\selectfont  {We call it this because we're "testing" our machine in the real world.}\par }{29}{section*.33}}
\@writefile{tdo}{\contentsline {todo}{\linespread  {0.9}\selectfont  {In this equation, we'll leave off $\theta $, to allow for non-parametric hypotheses.}\par }{30}{section*.34}}
\@writefile{loe}{\contentsline {kequation}{\numberline {22}Key Equation}{30}{kequation.22}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.6.6}Overfitting}{30}{subsection.1.6.6}}
\@writefile{loe}{\contentsline {definition}{\numberline {23}Definition}{30}{definition.23}}
\@writefile{toc}{\contentsline {section}{\numberline {1.7}Model Class}{30}{section.1.7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.7.1}Hypothesis Class}{31}{subsection.1.7.1}}
\@writefile{loe}{\contentsline {definition}{\numberline {24}Definition}{31}{definition.24}}
\@writefile{tdo}{\contentsline {todo}{\linespread  {0.9}\selectfont  {Another way to say "same type of equation" is "\textbf  {same functional form}".}\par }{31}{section*.35}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.7.2}Expressiveness}{31}{subsection.1.7.2}}
\@writefile{loe}{\contentsline {definition}{\numberline {25}Definition}{31}{definition.25}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.7.3}Choosing Model Classes}{31}{subsection.1.7.3}}
\@writefile{tdo}{\contentsline {todo}{\linespread  {0.9}\selectfont  {It's also more likely to overfit! We'll discuss why another time.}\par }{32}{section*.36}}
\@writefile{tdo}{\contentsline {todo}{\linespread  {0.9}\selectfont  {Research on this is ongoing: we continue to develop new model classes to try to better handle new and old problems!}\par }{32}{section*.37}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.7.4}Our Linear Model}{32}{subsection.1.7.4}}
\@writefile{loe}{\contentsline {notation}{\numberline {26}Notation}{32}{notation.26}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.7.5}Other Models}{33}{subsection.1.7.5}}
\@writefile{toc}{\contentsline {section}{\numberline {1.8}Algorithm}{33}{section.1.8}}
\@writefile{toc}{\contentsline {section}{\numberline {1.9}Overview of the Course}{33}{section.1.9}}
\@writefile{toc}{\contentsline {section}{\numberline {1.10}Terms}{34}{section.1.10}}
\@setckpt{chapters/intro}{
\setcounter{page}{36}
\setcounter{equation}{8}
\setcounter{enumi}{0}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{0}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{1}
\setcounter{section}{10}
\setcounter{subsection}{0}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{0}
\setcounter{table}{0}
\setcounter{@todonotes@numberoftodonotes}{37}
\setcounter{codelinenumber}{0}
\setcounter{indent}{0}
\setcounter{thisindent}{0}
\setcounter{Item}{0}
\setcounter{Hfootnote}{0}
\setcounter{bookmark@seq@number}{22}
\setcounter{tcbbreakpart}{0}
\setcounter{tcblayer}{0}
\setcounter{lstnumber}{1}
\setcounter{parentequation}{0}
\setcounter{ContinuedFloat}{0}
\setcounter{float@type}{16}
\setcounter{col}{0}
\setcounter{tkz@gr@a}{0}
\setcounter{tkz@gr@b}{0}
\setcounter{tkz@gr@c}{0}
\setcounter{tkz@gr@e}{0}
\setcounter{tkz@gr@d}{0}
\setcounter{tkz@gr@p}{0}
\setcounter{tkz@gr@i}{0}
\setcounter{tkz@gr@n}{0}
\setcounter{tkz@gr@ta}{0}
\setcounter{tkz@gr@tb}{0}
\setcounter{thmt@dummyctr}{26}
\setcounter{mdf@globalstyle@cnt}{1}
\setcounter{mdfcountframes}{0}
\setcounter{mdf@env@i}{0}
\setcounter{mdf@env@ii}{0}
\setcounter{mdf@zref@counter}{0}
\setcounter{theorem}{26}
\setcounter{section@level}{1}
\setcounter{lstlisting}{0}
}
