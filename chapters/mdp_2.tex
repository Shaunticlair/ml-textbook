\setcounter{chapter}{12-1} %Makes the prereq chapter chapter 0

\chapter{Markov Decision Processes 2 - Optimal Policies, Q-Values}

\setcounter{section}{1}


\section{Finding policies for MDPs}

    In the previous section, we computed the total value of different \purp{policies}: strategies for how to act, to get the \textbf{most rewards.}

    \begin{itemize}
        \item We designed \vocab{value functions} in order to \gren{evaluate} policies, and find the best one.
        \item So, we'll do that: we search for the \orgg{optimal} policies, in the \textbf{finite} and \textbf{infinite} cases.\\
    \end{itemize}

    \begin{definition}
        The \vocab{optimal policy} \pur{$\pi^*$} is better than (or as good as) every other policy \grn{$\pi$}, for \redd{every state}.

        \begin{equation*}
            \red{\forall s \in \mathcal{S}}: \qquad \qquad 
            V_{\grn{\pi}}(\red{s}) \leq V_{\pur{\pi^*}}(\red{s})
        \end{equation*}

        There can be \gren{multiple} optimal policies.
    \end{definition}

    

    \phantom{}

    \subsection{Optimal Policies -- Finite Horizon, $H=0,1$}

        We could try every possible policy, and compare their \gren{values} directly.

        \begin{itemize}
            \item But that's way too expensive: the number of policies is typically \purp{huge}. 
        \end{itemize}

        Instead, let's do what we did before: we start with the optimal policy for $h=0$, and build up a larger \vocab{horizon}.\\

        \begin{notation}
            Note that our \purp{policy} can depend on our \gren{horizon}. We'll add notation to accommodate this:

            \begin{itemize}
                \item The \vocab{optimal policy} for horizon $h$ is $\grn{\pi}_h^*$.
            \end{itemize}

            
        \end{notation}

        \miniex If it takes 10 steps to reach a very valuable state, you should have a different policy if 

        \begin{itemize}
            \item You have $h=3$ (not enough time to reach it)
            \item You have $h=100$ (more than enough time to reach it)
        \end{itemize}

        \subsecdiv

        First: $H=0$. There's no reward, no matter what we do: all policies are the same.\\

        \begin{concept}
            \purp{All policies} for $h=0$ are \vocab{optimal}.
        \end{concept}

        \subsecdiv

        Next, $H=1$: we only have to take one action. Thankfully, this is simple: we just take the \brow{action} that \purp{maximizes} our reward.

        \begin{itemize}
            \item This looks like a job for \vocab{argmax}.
                \note{In the regression chapter, we discussed argmin, but the principle is exactly the same.}\\
        \end{itemize}

        \begin{notation}
            \textit{Review from the Regression chapter:}
            
            The \vocab{argmax function} tells you the value of the \{input \brow{variable} that gives the \purp{maximum output}.
            
            \begin{equation*}
                \Theta^* = \argmax{ \pur{ \Theta } }{ \grn{ J(\Theta) } }
            \end{equation*}
            
            The \purp{function we want to maximize} is written to the right, while the \brow{variable we adjust} is written below.
        
        \end{notation}

        So, we want to know which \brow{action} maximizes the \purp{reward} function.

        \begin{itemize}
            \item We just compute this by comparing all the actions for a single \redd{state}.
        \end{itemize}

        \begin{equation}
            a^* = \argmax{\bro{a}}{ \Big( R\big( \red{s},\bro{a}\big) \Big) }
        \end{equation}

        If we're in state $\red{s}$, we want our \purp{optimal policy} to give this action.\\

        \begin{kequation}
            The \vocab{optimal policy} for horizon $H=1$ is:

            \begin{equation*}
                \pi^{*}_{1}\big(\red{s}\big) = \argmax{\bro{a}}{ \Big( R\big( \red{s},\bro{a}\big) \Big) }
            \end{equation*}
        \end{kequation}

            \note{Remember that we can have multiple optimal policies.}



    \phantom{}

    \subsection{Finite Horizon: $h=2$}

        Now, we want to find the policy if $H=2$.

        This has a few complications:

        \begin{itemize}
            \item We have to choose \purp{two} separate actions.
            
            \item Our \brow{$h=2$ action} will affect our state (and reward) at $h=1$.
                \begin{itemize}
                    \item So, we can't just choose the \textbf{action} that gives us the best \gren{immediate reward}: we need to account for \textit{future} reward.
                \end{itemize}
        \end{itemize}

        \subsecdiv


        This is what we designed our \vocab{value function} $V^h$ for! It allows us to compare policies, while accounting for \purp{future} actions/states/rewards.

        \begin{itemize}
            \item But we mentioned that there are \textbf{too many} possible policies to compare all of them. Is there a way we can \orgg{narrow it down}?
        \end{itemize} 

        Here's the trick: we use the same concept from before --\\

        \begin{concept}
            \textit{Review from chapter "Markov Decision Processes 1":}
            
            If we know our \redd{current state}, we actually \purp{don't care} about what happened during earlier timesteps.

            \begin{itemize}
                \item That means, if our current horizon is $h=1$, we can pretend as if we're starting a new MDP run with $H=1$, \orgg{ignoring} our original horizon.
            \end{itemize}
        \end{concept}

        Let's ignore whatever our first choice ($h=2$) was, or our state transition. All we know is that we ended up in state $\blu{s'}$.

        \begin{figure}[H]
            \centering
            \includegraphics[width=60mm,scale=0.4]{images/mdp_images/h2_forget.png}
            \caption*{Our problem is simpler if we simply "forget" our first step: we just pretend we started at $s'$.}
        \end{figure}

        \begin{itemize}
            \item Now we're in $h=1$. Thankfully, we already computed the optimal policy, $\pi_1^*$.

            \item Our reward will be 
                \note{Since we want the reward, not the action, we'll use "max", not "argmax".}

            \begin{equation}
                R\Big( \blu{s'},\pi_1^*(\blu{s'}) \Big) =
                \max_{\bro{a'}}{ \Big( R\big( \blu{s'},\bro{a'}\big) \Big) }
            \end{equation}

            \item Now, we know the \orgg{second half} of our optimal policy.\\
        \end{itemize}

        \begin{concept}
            The \vocab{optimal} choice of action is independent of previous actions: it's only based on our \redd{state} and \gren{horizon}.

            \begin{itemize}
                \item That means that "\textbf{the last \red{$h$} steps} of a horizon-\blu{$H$} MDP" is \gren{the same} as "\textbf{every step} of a horizon-\red{$h$} MDP"
                \item If we've already computed the optimal policy for a horizon-$h$ MDP, we can \orgg{re-use} them at the end of a \purp{longer horizon}.
            \end{itemize}

            \subsecdiv

            This \purp{simplifies} the search for our \vocab{optimal policy}: we can only focus on policies where those last $n$ steps \gren{match} policy $\pi_n^*$.
        \end{concept}

        \miniex Suppose you know the best way to finish a game of chess in $h$ turns, starting from any position.

        \begin{itemize}
            \item You can use that knowlegde earlier in the game: those will be the end of a longer, winning strategy, starting earlier.
                \note{One weakness of this analogy: in chess, we don't have a well-defined "horizon" for the end of the game. But the same general idea applies.}
        \end{itemize}

        Since we already know the best action, for each state, at $h=1$, we don't have to explore as many possible policies! 

        \subsecdiv

        Now, we can return to our $h=2$ step, with the knowledge of how we'll act in the future. 

        We'll re-introduce our value function, so we can figure out which policy is best:
 
        \begin{equation}
            V_{\grn{\pi}}^{\pur{2}}\big(\red{s} \big) =\quad 
                \overbrace{
                \pur{R} \Big( \red{s},\grn{\pi}\big(\red{s}\big) \Big)
                }^{h=2}
            \;\;+\;\;
                \sum_{\blu{s'}}  
                \;\;
                    \grn{T} \Big( \red{s},\grn{\pi}\big(\red{s}\big),\blu{s'} \Big)
                \;\cdot\; 
                \overbrace{
                \pur{R} \Big( \blu{s'},\grn{\pi}\big(\blu{s'}\big) \Big)
                }^{h=1}
        \end{equation}

        We'll adjust this: 
        
        \begin{itemize}
            \item Our current action is chosen by our policy $\grn{\pi_2}$.
            \item Our next action is chosen by the \orgg{maximum} reward for the $h=1$ step.
        \end{itemize}

        \begin{equation}
            \phantom{V_{\grn{\pi_2}}^{\pur{2}}\big(\red{s} \big) =}\quad 
                \overbrace{
                R \Big( s,\grn{\pi_2}\big(s\big) \Big)
                }^{h=2}
            \;\;+\;\;
                \sum_{s'}  
                \;\;
                    T \Big( s,\grn{\pi_2}\big(s\big),s' \Big)
                \;\cdot\; 
                \overbrace{
                \org{\max_{\bro{a'}}} \Big( R\big( s',\bro{a'}\big) \Big)  \Big)
                }^{h=1}
        \end{equation}



    \phantom{}

    \subsection{Q-Values}

        This is a bit different from $V$, though. Instead of using the \gren{same policy} for every step, we do something different:

        \begin{itemize}
            \item First, we take one \purp{chosen action} $\grn{\pi_2}\big(s\big)$
            \item Then for our \gren{second action}, we choose the optimal policy automatically.
        \end{itemize}

        We'll come up with a new name for this: a \vocab{Q-value function}.\\

        \begin{definition}
            A \vocab{Q-value function} $Q^h\big(\red{s},\bro{a}\big)$ (a.k.a "\vocab{state-action value function}") is \textbf{similar} to a value function $V^h_{\pi}\big(\red{s}\big)$, but instead of using the \gren{same policy} for every step, we:

            \begin{itemize}
                \item Choose one action $\bro{a}$
                \item Every choice afterwards is \purp{optimal}.
            \end{itemize}

            This is why the policy $\pi$ has been replaced by a single action $\bro{a}$: you only make \orgg{one choice}, and all following steps are optimal.

            \begin{itemize}
                \item $Q\big(\red{s},\bro{a}\big)$ tells us the \purp{expected reward}, under these conditions.
            \end{itemize}
            
        \end{definition}

        \begin{figure}[H]
            \centering
            \includegraphics[width=60mm,scale=0.4]{images/mdp_images/q_value.png}
            \caption*{We choose our first action, and the rest are optimal.}
        \end{figure}

        Despite appearing different, the $Q$-value function serves all of the roles we previously needed our value function $V$ for.\\

        \begin{concept}
            We can think of our $Q$-value function $Q^H\big(\red{s},\bro{a}\big)$ as a value function $V^H$ with a \vocab{special kind of policy:}

            \begin{equation*}
                Q^H\big(\red{s},\bro{a}\big) \longrightarrow 
                V^H_\pi\big(\red{s}\big)
            \end{equation*}

            This "special policy" now depends on \purp{horizon}.

            \begin{equation*}
                \pi_h(\red{s}) = 
                \begin{cases}
                    \bro{a} & h=H \\
                    \pi_h^*(\red{s}) & \text{otherwise}
                \end{cases}
            \end{equation*}
        \end{concept}

        This perfectly matches our current strategy: we try out one action $a$, and then rely on the \purp{optimal policies} we developed previously.
            \note{$Q$ can be seen as a way to "try out" one single action, to add onto your, so far, optimal policy.}

        \begin{equation}
            \phantom{V_{\grn{\pi_2}}^{\pur{2}}\big(\red{s} \big) =}\quad 
                \overbrace{
                R \Big( s,\grn{\pi_2}\big(s\big) \Big)
                }^{\text{One action}}
            \;\;+\;\;
                \sum_{s'}  
                \;\;
                    T \Big( s,\grn{\pi_2}\big(s\big),s' \Big)
                \;\cdot\; 
                \overbrace{
                \org{\max_{\pur{a'}}} \Big( R\big( s',\pur{a'}\big) \Big)  \Big)
                }^{\text{Optimal policies}}
        \end{equation}

        If we replace $\grn{\pi_2}\big(s\big)$ with $\bro{a}$, we have our $Q$-value function:

        \begin{equation}
            Q^{\pur{2}}\big(\red{s}, \bro{a} \big) =\quad 
                \overbrace{
                R \Big( s,\bro{a} \Big)
                }^{\text{One action}}
            \;\;+\;\;
                \sum_{s'}  
                \;\;
                    T \Big( s,\bro{a},s' \Big)
                \;\cdot\; 
                \overbrace{
                \org{\max_{\pur{a'}}} \Big( R\big( s',\pur{a'}\big) \Big)  \Big)
                }^{\text{Optimal policies}}
        \end{equation}


    \subsection{$H=2$ completed}

        Our new $Q$-value function already takes care of optimizing $h=1$, so now, we just need to optimize $h=2$: we need to find the \brow{best action} at $h=2$, $a^*$.

        \begin{equation}
            a^* = \bro{\argmax{a}}{} \Big( Q^{\pur{2}}\big(\red{s}, \bro{a} \big) \Big)
        \end{equation}

         \begin{kequation}
            We can use $Q^{\pur{2}}\big(\red{s}, \bro{a} \big)$ to determine the \vocab{optimal policy} for $H=2$.

            \begin{itemize}
                \item $Q^{\pur{2}}\big(\red{s}, \bro{a} \big)$ encodes information about \orgg{immediate} and \purp{future} rewards, while \gren{optimizing} those future steps.
            \end{itemize}
            
            
            This allows us to directly compare different actions $\bro{a}$, searching for an optimal policy:

            \begin{equation*}
                \grn{\pi_2^*}\big(\redd{s}\big) = 
                \bro{a^*} =
                \bro{\argmax{a}}{} \Big( 
                    Q^{\pur{2}}\big(\red{s}, \bro{a} \big) 
                \Big)
            \end{equation*}

            This action will be chosen for our optimal policy $\grn{\pi_2^*}$.
        \end{kequation}

    \subsection{$H=2$ Extended Solution (\redd{Optional})}

        Here's the un-compressed version: it's a lot messier, but it shows more of what's going on.\\

        \begin{remark}
            The \vocab{optimal policy} for horizon $H=2$ comes in two stages:

            \begin{itemize}
                \item We compute the \gren{optimal policy} $\pi_1^*$ and \purp{reward} for our second step, $h=1$.
                    \begin{itemize}
                        \item This tells us how \orgg{valuable} each $h=1$ state $\blu{s'}$ is.
                    \end{itemize}

                \item We compute the \brow{optimal action} $a^*$ for our first step, $h=1$, by factoring in

                    \begin{itemize}
                        \item The \orgg{immediate} reward, $\pur{R} \Big( \red{s},\grn{\pi}\big(\red{s}\big) \Big)$
                        \item The \purp{average rewards in the next step}, based on all possible outcomes.
                    \end{itemize}
            \end{itemize}

                \begin{equation*}
                    \grn{\pi_2^*}\big(\redd{s}\big) = a^* =
                    \bro{\argmax{a}}{} \Bigg\{
                        \overbrace{
                        R \Big( s,\bro{a} \Big)
                        }^{\text{Immediate Reward}}
                        \;\;+\;\;
                        \sum_{s'}  
                        \;\;
                            T \Big( s,\bro{a},s' \Big)
                        \;\cdot\; 
                        \overbrace{
                        \org{\max_{\pur{a'}}} \Big( R\big( s',\pur{a'}\big) \Big)  \Big)
                        }^{\text{(Optimized) Future Reward}}
                    \Bigg\}
                    \end{equation*}

        \end{remark}

        We can see that we \gren{separately} optimize our two steps:
        
        \begin{itemize}
            \item $\pi_1$ first, to get $\pi_1^*$: this gives us our action \bro{$a'$}.

            \begin{equation}
                \org{\max_{\bro{a'}}} \Big( R\big( \blu{s'},\bro{a'}\big) \Big) 
            \end{equation}
            
            \item Then, we use $\pi_1^*$ to find $\pi_2^*$: this gives us our action \pur{$a$}.

            \begin{equation}
                \pur{\argmax{a}{}} \Big( \dots \Big)
            \end{equation}
        \end{itemize}

    \subsection{$H=3$ and beyond}

        Introducing $Q$-values has given us the last tool we need to complete our finite-horizon MDP solution.

        Let's use everything we've built so far:

        \begin{itemize}
            \item After our first step, we're in state $\blu{s'}$, with $H=2$: we already determined what our policy should be, and what the \purp{value} of that policy is:
                \note{The value includes both $h=1$ and $h=2$.}
                
            \begin{equation}
                \grn{\pi_2^*}\big(\blu{s'}\big) = 
                \pur{\argmax{a'}}{} \Big( 
                    Q^{\pur{2}}\big(\blu{s'}, \pur{a'} \big) 
                \Big)
            \end{equation}

            \begin{equation} 
                \text{Optimal Value} = 
                \pur{\max_{a'}} \Big( 
                    Q^{\pur{2}}\big(\blu{s'}, \pur{a'} \big) 
                \Big)
            \end{equation}

            \item Our \purp{first step} is the only one we choose, giving us an immediate reward:

            \begin{equation}
                R \Big( s,\bro{a} \Big)
            \end{equation}
        \end{itemize}

        This matches our description for the $Q$-value function: all we have to do is account for different possible $\blu{s'}$ values, with $ T(s,a,s')$.

        \begin{equation}
            Q^{\pur{3}}\big(\red{s}, \bro{a} \big) 
            \;\;=\;\;
            R \Big( s,\bro{a} \Big)
            \;\;+\;\;
            \sum_{s'}  
                \;\;
                T \Big( s,\bro{a},s' \Big)
                \;\;\cdot\;\;
                \pur{\max_{a'}} \Big( 
                Q^{\pur{2}}\big(\blu{s'}, \pur{a'} \big) 
            \Big)
        \end{equation}

        This strategy, conceptually, works exactly the same, for any possible value of $H$:\\

        \begin{kequation}
            The $Q$-value function for horizon $H$ can be written as:

            

            \begin{equation*}
                Q^{\pur{H}}\big(\red{s}, \bro{a} \big)
                \;\;=\;\; 
                \overbrace{
                    \pur{R} \Big( \red{s},\bro{a} \Big)
                }^{\text{First reward}} 
                \quad+\quad
                \overbrace{
                \sum_{\blu{s'}}  
                        \;\;
                        \underbrace{
                            \grn{T} \Big(   
                            \red{s},\bro{a},\blu{s'} \Big)
                        }_{\text{Chance of } \red{s} \to \blu{s'}}
                        \;\;\; \cdot \;\;\; 
                        \underbrace{
                            \pur{\max_{a'}} \Big( 
                            Q^{\pur{H-1}}\big(\blu{s'}, \pur{a'} \big) \Big)
                        }_{\text{Optimize next step } }
                }^{\text{Future rewards}}
            \end{equation*}

            Or, without extra annotation:

            \begin{equation*}
                Q^{\pur{H}}\big(\red{s}, \bro{a} \big) 
                \;\;=\;\;
                R \Big( s,\bro{a} \Big)
                \;\;+\;\;
                \sum_{s'}  
                    \;\;
                    T \Big( s,\bro{a},s' \Big)
                    \;\;\cdot\;\;
                    \pur{\max_{a'}} \Big( 
                    Q^{\pur{H-1}}\big(\blu{s'}, \pur{a'} \big) 
                \Big)
            \end{equation*}

            Note that horizon $H$ relies on optimizing horizon $H-1$, similar to value iteration.
        \end{kequation}

        And based on this $Q$-value, we can determine the optimal action for our current state (and thus, our \purp{optimal policy}):\\

        \begin{kequation}
            We can use $Q^{\pur{H}}\big(\red{s}, \bro{a} \big)$ to determine the \vocab{optimal policy} for horizon $H$.

            \begin{equation*}
                \grn{\pi_H^*}\big(\redd{s}\big) = 
                \bro{\argmax{a}}{} \Big( 
                    Q^{\pur{H}}\big(\red{s}, \bro{a} \big) 
                \Big)
            \end{equation*}

            We can use this form of equation to get \orgg{every optimal policy}.
        \end{kequation}

        Note some important reminders. First, make sure to distinguish between $Q$ and $V$.\\

        \begin{clarification}
            Let's compare our \purp{value function} equation $V$, to our \gren{$Q$-value function} equation:

            \begin{itemize}
                \item For our $Q$-value, we choose \bro{one action}, and the remainder are \purp{optimal}.
            \end{itemize}

            \begin{equation*}
                Q^{\pur{H}}\big(\red{s}, \bro{a} \big) 
                \;\;=\;\;
                R \Big( s,\bro{a} \Big)
                \;\;+\;\;
                \sum_{s'}  
                    \;\;
                    T \Big( s,\bro{a},s' \Big)
                    \;\;\cdot\;\;
                    \pur{\max_{a'}} \Big( 
                    Q^{\pur{H-1}}\big(\blu{s'}, \pur{a'} \big) 
                \Big)
            \end{equation*}
            
            \begin{itemize}
                \item Meanwhile, $V$ relies on the \gren{same policy $\pi$} for all actions.
            \end{itemize}

        
            \begin{equation*}
                V_{\grn{\pi}}^{\pur{H}}\big(\red{s} \big) \;\;=\;\; 
                    \pur{R} \Big( \red{s},\grn{\pi}\big(\red{s}\big) \Big)
                \quad+\quad
                \sum_{\blu{s'}}  
                        \;\;
                            \grn{T} \Big(   
                            \red{s},\grn{\pi}\big(\red{s}\big),\blu{s'} \Big)
                        \;\;\; \cdot \;\;\; 
                            V_{\grn{\pi}}^{\pur{H-1}}\big(\blu{s'} \big)
            \end{equation*}

            
        \end{clarification}

        \begin{figure}[H]
            \centering
            \includegraphics[width=70mm,scale=0.4]{images/mdp_images/q_value.png}
            \caption*{Our first action is chosen manually: after that, we use an optimal policy.}
        \end{figure}

        \begin{figure}[H]
            \centering
            \includegraphics[width=60mm,scale=0.4]{images/mdp_images/v_value.png}
            \caption*{All of our actions are chosen by our (potentially non-optimal) policy $\pi$.}
        \end{figure}

    \pagebreak

    \subsection{Finite-Horizon $Q$-Value MDP solution}

        We have a method for creating the optimal policy:

        \begin{itemize}
            \item Choose any action for $\pi_0^*(s)$.
            \item Maximize $R(s,a)$ for $a=\pi_1^*(s)$
            \item Maximize $Q(s,a)$ for $a=\pi_h^*(s)$, if $h\geq 2$.
        \end{itemize}

        For horizon $H$, we can \orgg{re-use} the optimal policies for shorter horizons.

            \begin{itemize}
                \item This concept is encoded by the way $Q$-values work.

                \item So, we can use these $Q$-values to find the optimal policy:
            \end{itemize}

        

        So, our focus is on those $Q$-values. We'll compute them as we progressively increase the horizon:

        \begin{equation}
            \org{Q^{0}\big(s,a \big)}  \;\;=\;\; 
            0
        \end{equation}

        \begin{equation}
            \pur{Q^{1}\big(s,a \big)} \;\;=\;\; 
                    R \Big( s,\pi\big(s\big) \Big)
                + \org{0}
        \end{equation}

        \begin{equation}
            \red{Q^{2}\big(s,a \big)} \;\;=\;\; 
                    R \Big( s,a \Big)
                \;\;+\;\
                    \pur{\sum_{s'}}  
                        \;\;
                        T \Big(          s,a,s' \Big)
                        \;\cdot\; 
                        \bro{ \max_{a'} \Big( \pur{Q^{1}\big(s',a' \big)} \Big)}
        \end{equation}

        And if we keep going...

        \begin{equation}
            \grn{Q^{H}\big(s,a \big)} \;\;=\;\; 
                    R \Big( s,a \Big)
                \;\;+\;\;
                    \red{\sum_{s'}}  
                        \;\;
                        T \Big(          s,a,s' \Big)
                        \;\cdot\; 
                        \bro{ \max_{a'} \Big( \red{Q^{H-1}\big(s',a' \big)} \Big)}
        \end{equation}

    \subsection{Dynamic Programming}

        For value function $V$ and $Q$-values, we've been using a very particular strategy for computing our solutions.

        \begin{itemize}
            \item We solve \purp{simpler subproblems} (like $V^1$) and \gren{save the results}.
            \item Later, we \orgg{re-use} those solutions for more complicated problems (like $V^2$).
        \end{itemize}

        This saves us a lot of work: 
        
        \begin{itemize}
            \item \miniex We could re-compute all the $V^1$ values, every time we need a $V^2$ value.
            \item But if, instead, we just store those values and use them later, we save them.
            \item The benefits are more obvious for computing $V^{200}$: it would be a nightmare to have to consider every possible sub-problem.
                \note{We don't have to pay attention to $V^{2}$ when doing $V^{200}$: that information is stored in $V^{199}$.}
        \end{itemize}

        We call this strategy \vocab{dynamic programming}, despite it being neither "dynamic", nor "programming".\\

        \begin{definition}
            \vocab{Dynamic Programming} is a strategy for solving complex problems that can be broken up into simpler, similar problems ("subproblems").

            \begin{itemize}
                \item First, we solve the \purp{subproblems}, and \gren{save} the result. 

                \item These "mini-solutions" are re-used as a part of larger, more \textbf{complicated} problems.
            \end{itemize}

            Because a single sub-solution can be used \orgg{many times}, you save time by storing the answer, rather than re-computing it every time you need it.
            
        \end{definition}

        We use it for every step of our value/$Q$-value calculation: 

        \begin{equation}
            \grn{Q^{H}\big(s,a \big)} \;\;=\;\; 
                    R \Big( s,a \Big)
                \;\;+\;\;
                    \red{\sum_{s'}}  
                        \;\;
                        T \Big(          s,a,s' \Big)
                        \;\cdot\; 
                        \bro{ \max_{a'} \Big( \red{Q^{H-1}\big(s',a' \big)} \Big)}
        \end{equation}

        $Q^H$ always requires us to use $Q^{H-1}$.

        \begin{itemize}
            \item $Q^H(s,a)$ has an equation for each state-action pair $(s,a)$. 
            
            \begin{itemize}
                \item Each of these equations will use $Q^{H-1}$: that involves using $Q^{H-1}$ \purp{many times}.
            \end{itemize}
            
            \item It would be much slower if, for every $Q^H(s,a)$, we had to \gren{re-compute} all of $Q^{H-1}$.
        \end{itemize}



    \pagebreak

    \subsection{Dynamic Programming Performance (\redd{Optional})}

        Let's compare the performance of working with, and without dynamic programming. Our goal is to compute $Q^h(s,a)$.

        \begin{equation}
            |\mathcal{S}|=m, \qquad |\mathcal{A}|=n
        \end{equation}

        We'll need this concept:\\

        \begin{concept}
            If we have a pair of elements, $(a,b)$, the total number of \purp{possibilities} is:

            \begin{itemize}
                \item The number of possible $a$ values, \gren{times} the number of possible $b$ values.
            \end{itemize}

            This idea works for larger sequences: you just multiply together the \purp{possible choices} at each step.
        \end{concept}

        We'll count the amount of work we need with, and without dynamic programming.


        \subsecdiv

        First: \orgg{without dynamic programming}.

        One way to compute the value of $Q^H(s,a)$ is to consider \purp{every possible outcome} of that action, and find the optimal one.

        \begin{itemize}
            \item \gren{How many} outcomes are there? One outcome has one \textbf{state-action pair} ($mn$ pairs), for each \textbf{timestep} ($h$ timesteps).
                \note{$m$ states, $n$ actions: $mn$ pairs of states and actions.}
        \end{itemize}

        The total number of possible outcomes is $(mn)^h$. So, our total work is proportional to that:
            \note{At each step, we have $mn$ possible pairs. So, for two timesteps, we have to choose from $mn$ options, twice: $(mn)^2$.}

        \begin{equation}
            O\big((mn)^h\big)
        \end{equation}

        We won't teach $O$-notation here.

        \subsecdiv

        Let's \orgg{use dynamic programming} this time.

        \begin{itemize}
            \item For our \purp{final timestep}, we have $mn$ possible state-action pairs. We pick the optimal action for each state, and we \gren{save} its value as $Q^h(s,a)$.
            \item For our previous timestep, we \purp{don't care} about the possibilities in the final timestep: we just trust $Q^h(s,a)$, and use it to select one of our $mn$ actions. 
        \end{itemize}

        So, at each timestep, we compare $mn$ elements: we don't have to consider combinations across different timesteps.

        However, we do still have to do this calculation once per timestep: $h$ times. We get $mnh$.

        \begin{equation}
            O\big(mnh\big)
        \end{equation}

        \begin{concept}
            Using \vocab{dynamic programming} for $Q$-value computation dramatically increases \gren{efficiency}.

            Instead of taking $O\big((mn)^h\big)$ time, we need $O\big(mnh\big)$ time.

            \begin{itemize}
                \item That's \orgg{exponentially} faster!
            \end{itemize}
        \end{concept}

        In short: the difference is that dynamic programming allows us to "ignore" other timesteps, and just rely on $Q$-values.

        \begin{itemize}
            \item Without dynamic programming, we have to "remember" other timesteps, and consider \purp{combinations} of state-action pairs.
        \end{itemize}





    \pagebreak

    \subsection{Optimal Policies -- Infinite Horizon}

        Now, we need to figure out how to get the optimal \purp{infinite-horizon} policy.

        We'll start off with our finite $Q$-value equation:

        \begin{equation}
            \grn{Q^{H}\big(s,a \big)} \;\;=\;\; 
                    R \Big( s,a \Big)
                \;\;+\;\;
                    \red{\sum_{s'}}  
                        \;\;
                        T \Big(          s,a,s' \Big)
                        \;\cdot\; 
                        \bro{ \max_{a'} \Big( \red{Q^{H-1}\big(s',a' \big)} \Big)}
        \end{equation}

        All we do is \purp{remove the horizon}, and \gren{add the discount factor}. 

        \begin{itemize}
            \item In this situation, we take one action, and then take \purp{optimal} actions for every step after, forever (or until our model terminates).
            \item We notate this "optimal value" as $Q$.
        \end{itemize}

        \begin{equation}
            \pur{Q^*\big(s,a \big)} \;\;=\;\; 
                    R \Big( s,a \Big)
                \;\;+\;\;
                \gamma
                \pur{\sum_{s'}}  
                    \;\;
                    T \Big(          s,a,s' \Big)
                    \;\cdot\; 
                    \bro{ \max_{a'} \Big( \pur{Q^*\big(s',a' \big)} \Big)}
        \end{equation}

        \begin{definition}
            $Q^*\big(\red{s},\bro{a} \big)$ is the \vocab{total discounted reward}, assuming you:

            \begin{itemize}
                \item Take action $\bro{a}$ in state $\red{s}$.
                \item Choose the \gren{optimal action} for all future states.
            \end{itemize}
        \end{definition}

        
    
        

        Now, our $Q$-value function is an equation of \purp{itself}.\\

        \begin{kequation}
            The \vocab{$Q$-value function} for \purp{infinite horizon} can be written as:

            \begin{equation*}
                \pur{Q^*\big(s,a \big)} \;\;=\;\; 
                    R \Big( s,a \Big)
                \;\;+\;\;
                \gamma
                \pur{\sum_{s'}}  
                    \;\;
                    T \Big(          s,a,s' \Big)
                    \;\cdot\; 
                    \bro{ \max_{a'} \Big( \pur{Q^*\big(s',a' \big)} \Big)}
            \end{equation*}
        \end{kequation}

        Something similar happened before, when we were doing value functions. Why is that?

        \begin{itemize}
            \item We're in the \redd{same situation} before and after we take one (successful) step: we \purp{don't know} how long it will be until the model terminates.
            \item In fact, nothing has changed: the chance of failing after one more step is still $\gamma$.
        \end{itemize}

        Because "maximizing $Q$" is how we determine our policy, we see that:\\

        \begin{concept}
            The optimal policy for an \purp{infinite-horizon MDP} is \gren{the same}, at any timestep, regardless of past events.

            We call this a \vocab{stationary} optimal policy, because it doesn't change over time.
        \end{concept}

        Our goal is to look for one of these "stationary" optimal policies.
            \note{There can be several optimal policies. We're only looking for one.}


    \subsection{Finding an Optimal Policy: Value Iteration}

        We have a problem, though. When computing the \purp{infinite value function}, we were able to solve a system of \gren{linear equations}.

        \begin{itemize}
            \item But our $Q$-value function is \gren{non-linear} this time, because of the \brow{max} operation. We can't solve that!
        \end{itemize}

        Is there even a solution, in this complex system? It turns out there \purp{definitely is}:\\

        \begin{theorem}
            Our system of $Q$-values has a \purp{unique solution}.

            This allows us to compute an \gren{optimal policy}.
        \end{theorem}

            \note{There may be more than one optimal policy, but they'll all have the same, \gren{unique} $Q$-values.
            
            \phantom{}
            
            In other words, each policy is worth the same average reward.}

        \subsecdiv

        Instead of linear solving, we'll try something different:

        \begin{itemize}
            \item Our $Q$-value function could be said to have an "infinitely far" horizon, with a $1-\gamma$ chance of terminating every timestep.

            \item We could \purp{approximate} this by computing horizon $h=1,2,3,\dots$: as our horizon gets really big, we hope this is \purp{similar} to an infinite horizon.\\
         \end{itemize}

         \begin{concept}
             We want to approximate an \gren{infinite horizon} with a \vocab{really large, finite horizon}.

             To match the infinite case, we'll include a chance of \purp{termination}, $(1-\gamma)$.
         \end{concept}

         This is identical to our equations for finite horizon, but with a $\gamma$ term included.

        \begin{equation}
            \grn{Q^{H}\big(s,a \big)} \;\;=\;\; 
                    R \Big( s,a \Big)
                \;\;+\;\;
                    \red{\gamma}
                    \blu{\sum_{s'}}  
                        \;\;
                        T \Big(          s,a,s' \Big)
                        \;\cdot\; 
                        \bro{ \max_{a'} \Big( \blu{Q^{H-1}\big(s',a' \big)} \Big)}
        \end{equation}

        This is called \purp{value iteration}.

        However, we don't really care about the $H$ of our fake, \textbf{finite} horizon: our goal is to approximate the infinite-horizon $Q$.\\

        \begin{notation}
            For each step of our \vocab{value iteration} process, we'll distinguish between two parts of our $Q^*$ approximation:

            \begin{itemize}
                \item $Q_{old}$: the $H-1$ horizon: it's our \purp{previous}, less-accurate $Q^*$ approximation. 
                    \begin{itemize}
                        \item We use it to compute $Q_{new}$.
                    \end{itemize}

                \item $Q_{new}$: the $H$ horizon: it's our \gren{newest}, more accurate approximation.
                
            \end{itemize}

            \subsecdiv

            After each timestep, $Q_{new}$ \orgg{replaces} $Q_{old}$:

            \begin{itemize}
                \item Every approximation is used to create a new, better approximation. 
            \end{itemize}
        \end{notation}

        With this, we can properly represent $Q^*$.

        \begin{equation}
            \grn{Q_{new}\big(s,a \big)} \;\;=\;\; 
                    R \Big( s,a \Big)
                \;\;+\;\;
                    \red{\gamma}
                    \blu{\sum_{s'}}  
                        \;\;
                        T \Big(          s,a,s' \Big)
                        \;\cdot\; 
                        \bro{ \max_{a'} \Big( \blu{Q_{old}\big(s',a' \big)} \Big)}
        \end{equation}

        \begin{definition}
            \vocab{Infinite-horizon value iteration} is a process where we approximate the \purp{infinite horizon $Q$-value} $Q^*$ with a \gren{large, finite horizon $Q$-value} $Q^H$.

            To accomplish this, we compute $Q^H$, combined with a \purp{termination} chance $(p=1-\gamma)$, from the \textbf{infinite horizon} problem.

            \begin{itemize}
                \item Each $Q^h$ term is used to aim for a \orgg{better approximation} of $Q^*$ than the one before.
            \end{itemize}

            \subsecdiv

            It's more accurate to call these approximations of $Q^*$, rather than finite-horizon values $Q^h$. So, we'll change up our notation:

            \begin{itemize}
                \item We use $Q_{old}$ (our old approximation) to compute $Q_{new}$ (a new approximation), using the \gren{finite-horizon equation} (discounted with $\gamma$).
            \end{itemize}

            
        \end{definition}

        Throughout the process, we use the following equation:\\

        \begin{kequation}
            The equation for \vocab{infinite-horizon value iteration} is:

            \begin{equation*}
                \grn{Q_{new}\big(s,a \big)} \;\;=\;\; 
                        R \Big( s,a \Big)
                    \;\;+\;\;
                    \red{\gamma}
                    \blu{\sum_{s'}}  
                        \;\;
                        T \Big(          s,a,s' \Big)
                        \;\cdot\; 
                        \bro{ \max_{a'} \Big( \blu{Q_{old}\big(s',a' \big)} \Big)}
            \end{equation*}

            Once our $Q$ value is \gren{close to $Q^*$}, we can find an \purp{optimal policy}:
            
            \begin{equation*}
                Q_{new} \approx Q^* \qquad \implies \qquad 
                \grn{\pi^*}\big(\redd{s}\big) = 
                \bro{\argmax{a}}{} \Big( 
                    Q_{new}\big(\red{s}, \bro{a} \big) 
                \Big)
            \end{equation*}

            
        \end{kequation}

        \note{How do we know that value iteration converges at all? We have a \textbf{theorem} for that, in 12.2.14.}

        Next, we'll figure out when to \redd{stop} value iteration: when are we ready to use our $Q^*$ approximation?


    \pagebreak

    \subsection{Convergence of Value Iteration}

        How do we know when to stop running our value iteration? We want $Q_{new}$, to \purp{converge} to $Q^*$: it gets \gren{close} to the answer we want!

        How do we know \textit{when} we're converging?

        \begin{itemize}
            \item We can use gradient descent as an example:
                \note{We start at the blue x on the right. Each red line shows us "moving" to a new, better position.}
        \end{itemize}

        \begin{figure}[H]
            \centering
            \includegraphics[width=70mm,scale=0.5]{images/gradient_descent_images/good_converge_oscillate.png}

            \caption*{As we get \gren{close} to the solution, our steps have to become \purp{very small}. }   
        \end{figure}
            \note{If we're close to the solution and take a \purp{big} step, we'd get \gren{further away} again! So, our steps \redd{must} get smaller.}

        We can see a pattern as we converge:\\

        \begin{concept}
            As we get close to \vocab{convergence}, our sequence will typically \gren{stabilize}: the values become more and more \textbf{similar}.

            We can use this "slowing down" progress to \orgg{detect} convergence.
        \end{concept}
        
        Let's translate this to \vocab{value iteration}:

        \begin{itemize}
            \item As we converge, each $Q_{new}$ value will be \orgg{very close} to the previous one.

            \item So, we'll detect convergence by seeing when our approximation is \purp{changing by a very small amount}.\\
        \end{itemize}

        \begin{concept}
            We \vocab{terminate} our value-iteration process when our \orgg{newest approximations} $Q_{new}$ become \gren{very similar} to each other.

            If our approximations are similar (they change very little between timesteps), that means we're \purp{converging} to $Q^*$.
         \end{concept}

            \note{Even if $Q_{new}$ converges, how do we know it converges to $Q^*$? We have \textbf{another theorem} for that, in 12.2.14.}



    \pagebreak

    \subsection{Value Iteration: Termination Condition}

         Let's figure out how to represent this mathematically.

         \begin{itemize}
             \item We'll compare the two most recent approximations: $Q_{new}$ and $Q_{old}$.
             \item We want these two \orgg{value functions} to be \gren{similar}. How do we measure this?\\
         \end{itemize}

         \begin{concept}
                 To compare two functions, we see how different their \purp{outputs} are, for each \brow{input}.

                 \begin{equation*}
                     \Big|\Big|
                        f(x) - g(x)
                     \Big|\Big|
                 \end{equation*}
             \end{concept}

             So, we'll compare the values that we get for each $(s,a)$ pair.

             \begin{equation}
                \Big|\Big|
                    Q_{new}(\red{s},\bro{a}) - Q_{old}(\red{s},\bro{a})
                \Big|\Big|
             \end{equation}

         We want our value functions to be \gren{similar}: that means they need to be similar for \purp{all inputs}.
         
         \begin{itemize}
             \item We'll set a \gren{maximum} for how different each output can be, called $\eps$.\\
         \end{itemize}

         

         \begin{definition}
            We want to see when $Q_{new}$ and $Q_{old}$ are \gren{similar}.

            We'll say that they're similar if \purp{every output} is closer than a distance of $\eps$:

            \begin{equation*}
                \overbrace{
                    \red{\forall s}: \bro{\forall a}:
                    }^{\text{Every $(s,a)$ pair}}
                    \qquad \qquad
                \overbrace{
                    \Big|\Big|
                        Q_{new}(\red{s},\bro{a}) - Q_{old}(\red{s},\bro{a}) 
                    \Big|\Big|
                    < \eps
                }^{\text{...Must be at least this similar}}
             \end{equation*}
         \end{definition}
         
         If we want to check if \textbf{all} of them are similar, we can focus on the \redd{worst case}:

         \begin{itemize}
             \item Which $(s,a)$ makes them the \purp{most different}?
         \end{itemize}

         \begin{equation}
            \pur{\max_{s,a}}
            \Bigg(
            \Big|\Big|
                Q_{new}(\red{s},\bro{a}) - Q_{old}(\red{s},\bro{a})
            \Big|\Big|
            \Bigg)
         \end{equation}

         \begin{notation}
             When we need to take the maximum over \purp{multiple inputs}, we include \gren{both of them} underneath the \textbf{max} notation.

             \begin{equation*}
                 \max_{x,y} \Big( f\big(x,y\big) \Big)
             \end{equation*}

             The above asks, "if we can choose $x$ and $y$ to be anything, what's the \purp{largest value} of $f$ we can get?"
         \end{notation}

         This equation will tell us whether we're finished.\\

         \begin{kequation}
            The following equation tells us, "if we compare $Q_{new}$ to $Q_{old}$, what's the \purp{biggest difference} between their outputs?"
            
             \begin{equation*}
                 \pur{\max_{s,a}}
                    \Bigg(
                    \Big|\Big|
                        Q_{new}(\red{s},\bro{a}) - Q_{old}(\red{s},\bro{a})
                    \Big|\Big|
                    \Bigg)
             \end{equation*}

             \subsecdiv

             If their "biggest difference" is small $(< \eps)$, then you could say the two functions are \gren{similar} for every input.

             \begin{equation*}
                 \pur{\max_{s,a}}
                    \Bigg(
                    \Big|\Big|
                        Q_{new}(\red{s},\bro{a}) - Q_{old}(\red{s},\bro{a})
                    \Big|\Big|
                    \Bigg) < \eps
             \end{equation*}

             This is our \purp{termination condition}: once we reach this condition, our $Q^*$ approximation is "good enough".

             
         \end{kequation}

         Finally, we have a complete value-iteration process.\\

         \begin{definition}
             To do \vocab{$Q$-value iteration}, we follow these steps:

             \begin{enumerate}
                 \item Start with $Q_{new}=0$.
                 \item Set $Q_{old} = Q_{new}$. This moves us forward 1 timestep. 
                 \item Update $Q_{new}$.

                 \begin{equation*}
                    \grn{Q_{new}\big(s,a \big)} \;\;=\;\; 
                            R \Big( s,a \Big)
                        \;\;+\;\;
                        \red{\gamma}
                        \blu{\sum_{s'}}  
                            \;\;
                            T \Big(          s,a,s' \Big)
                            \;\cdot\; 
                            \bro{ \max_{a'} \Big( \blu{Q_{old}\big(s',a' \big)} \Big)}
                \end{equation*}

                \item Repeat step 2-3 until we \purp{converge}:

                    \begin{equation*}
                         \pur{\max_{s,a}}
                            \Bigg(
                            \Big|\Big|
                                Q_{new}(\red{s},\bro{a}) - Q_{old}(\red{s},\bro{a})
                            \Big|\Big|
                            \Bigg) < \eps
                     \end{equation*}

                 \item Return the optimal policy. 

                    \begin{equation*}
                        \grn{\pi^*}\big(\redd{s}\big) = 
                        \bro{\argmax{a}}{} \Big( 
                            Q^{*}\big(\red{s}, \bro{a} \big) 
                        \Big)
                    \end{equation*}
             \end{enumerate}
         \end{definition}

        If we write this as pseudocode:
         
        \begin{codebox}
        \Procname{$\proc{Infinite-Horizon-Value-Iteration}(\mathcal S, \mathcal A, T, R, \gamma, \epsilon)$}
        
        \li \For $\red{s \in \mathcal{S}}, \bro{a \in \mathcal{A}}:$ \qquad \qquad \# Every input pair
            \Do 
                \li $Q_{\text{old}}(\red{s}, \bro{a}) = 0$ \qquad \qquad \# Start from $Q^0=0$
            \End

        \li
            
        \li \While True: \# Continue until converged
        \li 
            \Do
                \li \For $s \in \mathcal{S}, a \in \mathcal{A}:$ \qquad \qquad \# Update $Q_{new}$
                \Do
                    \li
                    \li $Q_{\text{new}} \big(\red{s}, \bro{a} \big) =       R \big( \red{s}, \bro{a} \big) \;\;+\;\; 
                    \gamma\sum_{s'} \;\;
                        T \Big(\red{s}, \bro{a}, \blu{s'} \Big)
                    \;\;\cdot\;\; \pur{\max_{a'}}
                        \Big( Q_{\text{old}} \big( \blu{s'}, \pur{a'} \big) \Big)$
                \End
        \li
        
        \li \If $\pur{\max_{s,a}}
                            \Big(
                            \big|\big|
                                Q_{new}(\red{s},\bro{a}) - Q_{old}(\red{s},\bro{a})
                            \big|\big|
                            \Big) < \eps$  \qquad \qquad \#If converged, we're finished
            \Do
                \li return $Q_{\text{new}}$
            \End
        \li
        
        \li $Q_{\text{old}} = Q_{\text{new}}$ \qquad \qquad \#Re-use for next iteration
        \End
        \end{codebox}



    \pagebreak
    
    \subsection{Convergence Theorems}

        We've made a couple pretty large \vocab{assumptions}: thankfully, each one has a \purp{theoretical} justification.

        In order to discuss these theorems, we need to clarify some notation:\\

        \begin{notation}
            Suppose we have run our value iteration for $n$ steps (not necessarily enough for convergence within $\eps$). For simplicity, let's say $Q=Q_{new}$: it's our current best approximation of $Q^*$.

            Our policies:

            \begin{itemize}
                \item $\pi^*$ is the optimal policy, based on $Q^*$.
                \item $\pi_Q$ guesses the optimal policy, based on $Q$.
            \end{itemize}

            Our value functions:

            \begin{itemize}
                \item $V_{\pi_Q}$ is the reward of using $\pi_Q$ forever.

                \item $V_{\pi^*}$ is the reward of using $\pi^*$ forever.
            \end{itemize}
        \end{notation}

        If we succeed, then $Q\approx Q^*$, and $\pi_Q = \pi^*$.

        \subsecdiv

        Our first assumption: "value-iteration makes our approximation better".\\

        \begin{theorem}

            \begin{equation*}
                 \pur{\max_{s}}
                    \Bigg(
                    \Big|\Big|
                        V_{\pi_Q}(\red{s}) - V_{\pi^*}(\red{s})
                    \Big|\Big|
                    \Bigg)
                \qquad 
                    \text{ never increases}
             \end{equation*}

             or,

             \begin{equation*}
                \overbrace{
                 \pur{\max_{s}}
                    \Bigg(
                    \Big|\Big|
                        V_{\pi_Q}(\red{s}) - V_{\pi^*}(\red{s})
                    \Big|\Big|
                    \Bigg)
                    }^{\text{Our worst-performing state $\pi(s)$}}
                \qquad 
                \overbrace{
                    \text{ never increases}
                }^{\text{Never gets worse than this}}
             \end{equation*}

             

             By taking the max, we're getting the "worst" performing state for $\pi_Q$: we'll call this $\red{s_{bad}}$. 

             \begin{itemize}
                 \item This sets a \orgg{limit} on how bad our model can be. 
                 \item Iteration can't make any state worse than $\red{s_{bad}}$ currently is.
                 \item $\red{s_{bad}}$ can only stay the same, or get better!
             \end{itemize}
             
        \end{theorem}

            \note{Another way to say "never increases" is "decreases monotonically".}

        This seems pretty weak: it's nice to know that value-iteration can't make $\pi_Q$ much worse, but how do we know it gets better? How do we know it converges?\\

        \begin{theorem}
            When we run \vocab{value iteration} with $\epsilon$, we will eventually \gren{improve} $Q$ enough to meet our requirement: 
            
            \begin{equation*}
                \text{After value iteration, } \qquad
                \overbrace{
                \pur{\max_{s}}
                    \Bigg(
                    \Big|\Big|
                        V_{\pi_Q}(\red{s}) - V_{\pi^*}(\red{s})
                    \Big|\Big|
                    \Bigg) < \eps
                }^{\text{$Q$ will approach $Q^*$ (within distance $\eps$)}}
            \end{equation*}

            This means that if we run value iteration long enough, it \purp{will converge}, and get as close to $Q^*$ as we want.
        \end{theorem}

        That's great! $Q$ approaches $Q^*$, when we run value iteration.

        We just have one more problem: just because $Q$ is \purp{close} to $Q^*$, doesn't mean they're close enough to get the optimal policy we want, $\pi^*$.

        \begin{itemize}
            \item Can get that policy?\\
        \end{itemize}

        \begin{theorem}
            If we choose the right $\eps$, our policy $\pi_Q$ \purp{will be} the optimal policy $\pi^*$.

            \begin{equation*}
                \overbrace{
                    \org{\exists \eps>0}:
                }^{\text{There is an $\eps$ where}}
                \qquad 
                \overbrace{
                \pi_Q = \pi^*
                }^{\text{We find the optimal policy}}
            \end{equation*}
        \end{theorem}

        We don't know what value of $\eps$ is required, but it's good to know that it always exists: we can always find the optimal policy.

        \subsecdiv

        One last useful computational trick:\\

        \begin{concept}
            We can run the different parts of value-iteration \gren{in parallel}, out-of-sync with each other.

            As long as we update each $(s,a)$ "infinitely many times" (as many times as we need), we will still \purp{converge}.
        \end{concept}


\section{Terms}

    \subsection*{Section 12.0}

    \begin{itemize}
        \item Timestep $t$ (Review)
        \item State $s_t$
        \item Input $x_t$
        \item Transition function $f_s$
        \item Output $y_t$
        \item Output function $f_o$
        \item State Machine
        \item Finite State Machine
        \item State Transition Diagram
        \item Linearity 
        \item Time-Invariance
        \item Linear Time-Invariant System (LTI)
        \item Actions $a_t$
        \item Deterministic State Machine
        \item Probabilistic State Machine
        \item Reward function $R(s,a)$
        \item State-action pair
        \item Stochastic (Review)
    \end{itemize}

    \subsection*{Section 12.1}
    \begin{itemize}
        \item Action space
        \item State space
        \item Transition Model $T(s,a,s')$
        \item (Probabilistic) State-Transition Diagram
        \item Transition Matrix $\mathcal{T}(a)$
        \item Markov Decision Process $(\mathcal{S},\mathcal{A},T,R,\gamma)$
        \item Policy $\pi(s)$
        \item Value Function
        \item Finite Horizon
        \item Horizon $h$
        \item Timestep $t$ (Review)
        \item MDP Termination
        \item Finite Horizon Value Function $V_{\pi}^h(s)$
        \item Expected Value
        \item Weighted Average (Review)
        \item Probability Distribution (Review)
        \item Infinite Horizon
        \item Infinite Horizon Value Function $V_{\pi}(s)$
        \item Discounting
        \item Discount Factor
        \item Discounted Average
        \item MDP Lifespan
        \item Memorylessness 
        \item Markov Property (Optional)
        
    \end{itemize}

    \subsection*{Section 12.2}
    \begin{itemize}
        \item Optimal Policy
        \item Argmax (Review)
        \item $Q$-Value Function (State-action value function)
        \item Dynamic Programming
        \item Value Iteration
        \item Convergence (Review)
    \end{itemize}
    

        



        

        

    

        

    

        


        

        

        

        

    


