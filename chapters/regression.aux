\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\zref@newlabel[2]{}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Regression}{2}{chapter.2}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:regression}{{2}{2}{Regression}{chapter.2}{}}
\newlabel{chap:regression@cref}{{[chapter][2][]2}{2}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Problem Formulation}{2}{section.2.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Hypothesis (Review)}{2}{subsection.2.1.1}}
\@writefile{tdo}{\contentsline {todo}{\linespread  {0.9}\selectfont  {Often, people will treat $\Theta $ and $h$ as almost interchangeable: be careful when you're doing this!}\par }{3}{section*.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}The Problem of Regression}{3}{subsection.2.1.2}}
\@writefile{tdo}{\contentsline {todo}{\linespread  {0.9}\selectfont  {Remember the notation for real-numbered vectors we introduced in the last chapter!}\par }{3}{section*.2}}
\@writefile{loe}{\addvspace {10\p@ }}
\@writefile{loe}{\contentsline {notation}{\numberline {1}Notation}{3}{notation.1}}
\@writefile{tdo}{\contentsline {todo}{\linespread  {0.9}\selectfont  {Technically, a \textbf  {space} is a set "with \textbf  {added} structure", which is about as broad as it sounds.}\par }{3}{section*.3}}
\@writefile{loe}{\contentsline {definition}{\numberline {2}Definition}{3}{definition.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.3}Converting our data}{4}{subsection.2.1.3}}
\@writefile{loe}{\contentsline {definition}{\numberline {3}Definition}{4}{definition.3}}
\@writefile{tdo}{\contentsline {todo}{\linespread  {0.9}\selectfont  {This particular feature transformation is called \textbf  {one-hot encoding}! We'll return to it later.}\par }{4}{section*.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.4}Our dataset}{5}{subsection.2.1.4}}
\@writefile{tdo}{\contentsline {todo}{\linespread  {0.9}\selectfont  {Remember that the superscript tells us that this is the 1st data point.}\par }{5}{section*.6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.5}Measuring our performance}{5}{subsection.2.1.5}}
\@writefile{loe}{\contentsline {kequation}{\numberline {4}Key Equation}{5}{kequation.4}}
\@writefile{loe}{\contentsline {clarification}{\numberline {5}Clarification}{6}{clarification.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.6}Learning to Generalize}{6}{subsection.2.1.6}}
\@writefile{loe}{\contentsline {kequation}{\numberline {6}Key Equation}{6}{kequation.6}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Regression as an optimization problem}{7}{section.2.2}}
\@writefile{tdo}{\contentsline {todo}{\linespread  {0.9}\selectfont  {Lot of research has gone into solving optimization problems!}\par }{7}{section*.7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Objective Function}{7}{subsection.2.2.1}}
\@writefile{tdo}{\contentsline {todo}{\linespread  {0.9}\selectfont  {In later sections, we will introduce something called a \textbf  {regularizer} to do just that!}\par }{7}{section*.8}}
\@writefile{loe}{\contentsline {definition}{\numberline {7}Definition}{7}{definition.7}}
\@writefile{tdo}{\contentsline {todo}{\linespread  {0.9}\selectfont  {We will be seeing a lot of $\Theta $ for a while.}\par }{7}{section*.9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Parts of an Objective Function}{7}{subsection.2.2.2}}
\@writefile{loe}{\contentsline {kequation}{\numberline {8}Key Equation}{8}{kequation.8}}
\@writefile{loe}{\contentsline {clarification}{\numberline {9}Clarification}{8}{clarification.9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.3}Minimization Notation}{8}{subsection.2.2.3}}
\@writefile{loe}{\contentsline {notation}{\numberline {10}Notation}{9}{notation.10}}
\@writefile{loe}{\contentsline {notation}{\numberline {11}Notation}{9}{notation.11}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.4}Optimal Value Notation}{9}{subsection.2.2.4}}
\@writefile{loe}{\contentsline {notation}{\numberline {12}Notation}{9}{notation.12}}
\@writefile{loe}{\contentsline {kequation}{\numberline {13}Key Equation}{10}{kequation.13}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Linear Regression}{11}{section.2.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}The Linear Model, 1-D}{11}{subsection.2.3.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}The Linear Model, 2-D}{11}{subsection.2.3.2}}
\@writefile{tdo}{\contentsline {todo}{\linespread  {0.9}\selectfont  {If $\theta _1$ is the "slope" for $x_1$, $\theta _2$ is the "slope" for $x_2$.}\par }{11}{section*.10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.3}The Linear Model, $d$-D}{11}{subsection.2.3.3}}
\@writefile{tdo}{\contentsline {todo}{\linespread  {0.9}\selectfont  {This is the "dimension" of our input space: the \textbf  {number} of input variables we have.}\par }{11}{section*.11}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.4}The Linear Model using Vectors}{12}{subsection.2.3.4}}
\@writefile{loe}{\contentsline {notation}{\numberline {14}Notation}{12}{notation.14}}
\@writefile{tdo}{\contentsline {todo}{\linespread  {0.9}\selectfont  {One benefit it that we can use \textbf  {matrices} instead of just \textbf  {vectors}!}\par }{12}{section*.12}}
\@writefile{loe}{\contentsline {kequation}{\numberline {15}Key Equation}{12}{kequation.15}}
\@writefile{tdo}{\contentsline {todo}{\linespread  {0.9}\selectfont  {Make sure you know what $\theta ^T$ is: it's the \textbf  {transpose}!}\par }{12}{section*.13}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.5}Regression Loss}{12}{subsection.2.3.5}}
\@writefile{tdo}{\contentsline {todo}{\linespread  {0.9}\selectfont  {The slope increases, so the loss grows faster and faster!}\par }{13}{section*.14}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.6}Our Goal}{13}{subsection.2.3.6}}
\@writefile{loe}{\contentsline {kequation}{\numberline {16}Key Equation}{14}{kequation.16}}
\@writefile{tdo}{\contentsline {todo}{\linespread  {0.9}\selectfont  {We now have two parameters in our argmin function, but aside from listing both of them, the notation is the same. We just substituted $\Theta = (\theta , \theta _0)$}\par }{14}{section*.15}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.7}Visualizing our Model}{14}{subsection.2.3.7}}
\@writefile{loe}{\contentsline {definition}{\numberline {17}Definition}{15}{definition.17}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.8}Another Interpretation}{15}{subsection.2.3.8}}
\@writefile{loe}{\contentsline {definition}{\numberline {18}Definition}{16}{definition.18}}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}The stupidest possible linear regression algorithm}{17}{section.2.4}}
\@writefile{toc}{\contentsline {section}{\numberline {2.5}Analytical solution: ordinary least squares}{18}{section.2.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.1}Trying to Simplify}{18}{subsection.2.5.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.2}Combining $\theta $ and $\theta _0$}{18}{subsection.2.5.2}}
\@writefile{tdo}{\contentsline {todo}{\linespread  {0.9}\selectfont  {We drop the $^{(i)}$ notation whenever it isn't necessary, to de-clutter the equations. We only do this when we don't care which data point we're using.}\par }{18}{section*.19}}
\@writefile{tdo}{\contentsline {todo}{\linespread  {0.9}\selectfont  {You can always factor out 1 without changing the value!}\par }{18}{section*.20}}
\@writefile{loe}{\contentsline {concept}{\numberline {19}Concept}{19}{concept.19}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.3}Summing over data points}{19}{subsection.2.5.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.4}Summing with Vectors: Row Vectors}{20}{subsection.2.5.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.5}Going from $x$ to $X$}{21}{subsection.2.5.5}}
\@writefile{loe}{\contentsline {notation}{\numberline {20}Notation}{21}{notation.20}}
\@writefile{tdo}{\contentsline {todo}{\linespread  {0.9}\selectfont  {Still in the 1D case!}\par }{21}{section*.21}}
\@writefile{loe}{\contentsline {concept}{\numberline {21}Concept}{21}{concept.21}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.6}Putting it together: Matrices}{22}{subsection.2.5.6}}
\@writefile{tdo}{\contentsline {todo}{\linespread  {0.9}\selectfont  {We'll leave off the appended 1 for now.}\par }{22}{section*.22}}
\@writefile{loe}{\contentsline {kequation}{\numberline {22}Key Equation}{22}{kequation.22}}
\@writefile{loe}{\contentsline {kequation}{\numberline {23}Key Equation}{23}{kequation.23}}
\@writefile{loe}{\contentsline {kequation}{\numberline {24}Key Equation}{23}{kequation.24}}
\@writefile{loe}{\contentsline {concept}{\numberline {25}Concept}{23}{concept.25}}
\@writefile{tdo}{\contentsline {todo}{\linespread  {0.9}\selectfont  {Notice that these shapes make sense for our above equation! Try working through the matrix multiplication to verify this.}\par }{23}{section*.23}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.7}Alterate Notation}{23}{subsection.2.5.7}}
\@writefile{loe}{\contentsline {notation}{\numberline {26}Notation}{24}{notation.26}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.8}Optimization in 1-D - Calculus Returns!}{24}{subsection.2.5.8}}
\@writefile{tdo}{\contentsline {todo}{\linespread  {0.9}\selectfont  {Assuming a "smooth" surface...}\par }{24}{section*.24}}
\@writefile{tdo}{\contentsline {todo}{\linespread  {0.9}\selectfont  {We technically need to prove whether this is minimum, maximum, or neither. For now, we'll assume we have a minimum.}\par }{24}{section*.25}}
\@writefile{loe}{\contentsline {concept}{\numberline {27}Concept}{25}{concept.27}}
\@writefile{tdo}{\contentsline {todo}{\linespread  {0.9}\selectfont  {This concept is review from 18.01 (Single-variable calculus), but is worth repeating!}\par }{25}{section*.26}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.9}Using our sum}{25}{subsection.2.5.9}}
\@writefile{tdo}{\contentsline {todo}{\linespread  {0.9}\selectfont  {Check the prerequisites chapter, chapter 0, for a full definition of linearity.}\par }{25}{section*.27}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.10}Optimizing for multiple variables}{25}{subsection.2.5.10}}
\@writefile{tdo}{\contentsline {todo}{\linespread  {0.9}\selectfont  {We'll ignore the averaging and $^{(i)}$ notation since that's easy to add on afterwards.}\par }{25}{section*.28}}
\@writefile{tdo}{\contentsline {todo}{\linespread  {0.9}\selectfont  {This approximation formula becomes exact as the step size shrinks: we go from $\Delta \theta $ to $\dd \theta $.}\par }{26}{section*.29}}
\@writefile{loe}{\contentsline {concept}{\numberline {28}Concept}{26}{concept.28}}
\@writefile{tdo}{\contentsline {todo}{\linespread  {0.9}\selectfont  {Again, we ignore the second requirement of making sure this isn't a \textbf  {maximum} or \textbf  {saddle} point.}\par }{26}{section*.30}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.11}Gradient Notation}{26}{subsection.2.5.11}}
\@writefile{loe}{\contentsline {kequation}{\numberline {29}Key Equation}{26}{kequation.29}}
\@writefile{tdo}{\contentsline {todo}{\linespread  {0.9}\selectfont  {Note the subscript on the gradient! This emphasizes that our \textbf  {space} is the components of $\theta $, not the components of our data $x$.}\par }{26}{section*.31}}
\@writefile{loe}{\contentsline {concept}{\numberline {30}Concept}{27}{concept.30}}
\@writefile{tdo}{\contentsline {todo}{\linespread  {0.9}\selectfont  {Ignoring the requirement from earlier! We're assuming it's a minimum.}\par }{27}{section*.32}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.12}Matrix Calculus}{27}{subsection.2.5.12}}
\@writefile{tdo}{\contentsline {todo}{\linespread  {0.9}\selectfont  {There's a document explaining vector derivatives coming soon!}\par }{27}{section*.33}}
\@writefile{tdo}{\contentsline {todo}{\linespread  {0.9}\selectfont  {Sometimes, you can guess a derivative by using the familiar rules and fixing shape errors with transposing/changing multiplication order. But be careful!}\par }{27}{section*.34}}
\@writefile{loe}{\contentsline {kequation}{\numberline {31}Key Equation}{28}{kequation.31}}
\@writefile{toc}{\contentsline {section}{\numberline {2.6}Regularization}{29}{section.2.6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.1}Overfitting (Review)}{29}{subsection.2.6.1}}
\@writefile{loe}{\contentsline {definition}{\numberline {32}Definition}{29}{definition.32}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.2}Regularizers}{29}{subsection.2.6.2}}
\@writefile{loe}{\contentsline {definition}{\numberline {33}Definition}{29}{definition.33}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.3}Lambda, a.k.a. $\lambda $}{30}{subsection.2.6.3}}
\@writefile{loe}{\contentsline {definition}{\numberline {34}Definition}{30}{definition.34}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.4}Tradeoffs: Estimation Error}{30}{subsection.2.6.4}}
\@writefile{loe}{\contentsline {definition}{\numberline {35}Definition}{30}{definition.35}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.5}Tradeoffs: Structural Error}{31}{subsection.2.6.5}}
\@writefile{loe}{\contentsline {definition}{\numberline {36}Definition}{31}{definition.36}}
\@writefile{tdo}{\contentsline {todo}{\linespread  {0.9}\selectfont  {Remember that \textbf  {expressiveness} is about how many possible models you have: if you have more models, you can solve more problems.}\par }{31}{section*.35}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.6}Tradeoffs of $\lambda $}{31}{subsection.2.6.6}}
\@writefile{loe}{\contentsline {concept}{\numberline {37}Concept}{31}{concept.37}}
\@writefile{loe}{\contentsline {concept}{\numberline {38}Concept}{32}{concept.38}}
\@writefile{toc}{\contentsline {section}{\numberline {2.7}Evaluating Learning Algorithms}{33}{section.2.7}}
\@writefile{toc}{\contentsline {section}{\numberline {2.8}Old Materials}{33}{section.2.8}}
\@writefile{toc}{\contentsline {section}{\numberline {2.9}Analytical solution: ordinary least squares}{33}{section.2.9}}
\@writefile{toc}{\contentsline {section}{\numberline {2.10}Regularization}{33}{section.2.10}}
\newlabel{sec:regularization}{{2.10}{33}{Regularization}{section.2.10}{}}
\newlabel{sec:regularization@cref}{{[section][10][2]2.10}{33}}
\@writefile{tdo}{\contentsline {todo}{\linespread  {0.9}\selectfont  {Learn about Bayesian methods in machine learning to see the theory behind this and cool results!}\par }{33}{section*.36}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.10.1}Ridge regression}{34}{subsection.2.10.1}}
\newlabel{sec:ridge_regression}{{2.10.1}{34}{Ridge regression}{subsection.2.10.1}{}}
\newlabel{sec:ridge_regression@cref}{{[subsection][1][2,10]2.10.1}{34}}
\@writefile{loe}{\contentsline {question}{\numberline {39}Question}{34}{question.39}}
\@writefile{loe}{\contentsline {question}{\numberline {40}Question}{34}{question.40}}
\@writefile{tdo}{\contentsline {todo}{\linespread  {0.9}\selectfont  {Remember that $I$ stands for the identity matrix, a square matrix that has 1's along the diagonal and 0's everywhere else.}{\@@par }}{34}{section*.37}}
\newlabel{eq:ridge_regression_solution}{{2.56}{34}{}{equation.2.10.56}{}}
\newlabel{eq:ridge_regression_solution@cref}{{[equation][56][2]2.56}{34}}
\@writefile{tdo}{\contentsline {todo}{\linespread  {0.9}\selectfont  {This is called ``ridge'' regression because we are adding a ``ridge'' of $n\lambda $ values along the diagonal of the matrix before inverting it.}{\@@par }}{34}{section*.38}}
\@writefile{loe}{\contentsline {question}{\numberline {41}Question}{35}{question.41}}
\@writefile{toc}{\contentsline {section}{\numberline {2.11}Evaluating learning algorithms}{35}{section.2.11}}
\newlabel{sec:reg_learn_alg}{{2.11}{35}{Evaluating learning algorithms}{section.2.11}{}}
\newlabel{sec:reg_learn_alg@cref}{{[section][11][2]2.11}{35}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.11.1}Evaluating hypotheses}{35}{subsection.2.11.1}}
\@writefile{tdo}{\contentsline {todo}{\linespread  {0.9}\selectfont  {It's a bit funny to interpret the analytical formulas given above for $\theta $ as ``training,'' but later when we employ more statistical methods ``training'' will be a meaningful concept.}{\@@par }}{35}{section*.39}}
\@writefile{tdo}{\contentsline {todo}{\linespread  {0.9}\selectfont  {There are technical definitions of these concepts that are studied in more advanced treatments of machine learning. Structural error is referred to as {\em  bias} and estimation error is referred to as {\em  variance}.}{\@@par }}{35}{section*.40}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.11.2}Evaluating learning algorithms}{36}{subsection.2.11.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.11.2.1}Validation}{36}{subsubsection.2.11.2.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.11.2.2}Cross validation}{36}{subsubsection.2.11.2.2}}
\newlabel{cross-validation}{{2.11.2.2}{36}{Cross validation}{subsubsection.2.11.2.2}{}}
\newlabel{cross-validation@cref}{{[subsubsection][2][2,11,2]2.11.2.2}{36}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.11.2.3}Hyperparameter tuning}{37}{subsubsection.2.11.2.3}}
\@writefile{loe}{\contentsline {question}{\numberline {42}Question}{37}{question.42}}
\@setckpt{chapters/regression}{
\setcounter{page}{38}
\setcounter{equation}{56}
\setcounter{enumi}{0}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{0}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{2}
\setcounter{section}{11}
\setcounter{subsection}{2}
\setcounter{subsubsection}{3}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{0}
\setcounter{table}{0}
\setcounter{@todonotes@numberoftodonotes}{36}
\setcounter{codelinenumber}{5}
\setcounter{indent}{0}
\setcounter{thisindent}{0}
\setcounter{Item}{0}
\setcounter{Hfootnote}{0}
\setcounter{bookmark@seq@number}{12}
\setcounter{tcbbreakpart}{0}
\setcounter{tcblayer}{0}
\setcounter{lstnumber}{1}
\setcounter{parentequation}{0}
\setcounter{ContinuedFloat}{0}
\setcounter{float@type}{16}
\setcounter{col}{0}
\setcounter{tkz@gr@a}{0}
\setcounter{tkz@gr@b}{0}
\setcounter{tkz@gr@c}{0}
\setcounter{tkz@gr@e}{0}
\setcounter{tkz@gr@d}{0}
\setcounter{tkz@gr@p}{0}
\setcounter{tkz@gr@i}{0}
\setcounter{tkz@gr@n}{0}
\setcounter{tkz@gr@ta}{0}
\setcounter{tkz@gr@tb}{0}
\setcounter{thmt@dummyctr}{42}
\setcounter{mdf@globalstyle@cnt}{1}
\setcounter{mdfcountframes}{0}
\setcounter{mdf@env@i}{0}
\setcounter{mdf@env@ii}{0}
\setcounter{mdf@zref@counter}{0}
\setcounter{theorem}{42}
\setcounter{section@level}{3}
\setcounter{lstlisting}{0}
}
