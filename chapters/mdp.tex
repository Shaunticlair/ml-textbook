\chapter{Markov Decision Processes}
\label{chap:mdps}

In Chapter~\ref{chap:rnn} we considered state machines that are deterministic
in their state transitions. 
A {\em Markov decision process}\index{Markov decision process} ({\sc mdp}) is a variation on a state
machine in which:
\begin{itemize}
\item The transition function is {\em stochastic},
\note{Recall that stochastic is another word for {\em probabilistic};
  we don't say ``random'' because that can be interpreted in two ways,
  both of which are incorrect.  We don't pick the transition function
  itself at random from a distribution.  The transition function
  doesn't pick its output {\em uniformly} at random.}
meaning that it defines a probability distribution over the  next
state given the previous state and input, but each time it is
evaluated it draws a new state from that distribution.
\item The output is equal to the state (that is $g$ is the identity
  function).  
%\note{There is an interesting variation on {\sc mdp}s,
%    called a {\em partially observable} {\sc mdp}, in which the output
%    is also drawn from a distribution depending on the state.}
\item Some states (or state-action pairs) are more desirable than others.
\end{itemize}

An {\sc mdp} can be used to model interaction with an outside
``world,'' such as a single-player \note{And there is an interesting,
  direct extension to two-player zero-sum games, such as Chess and
  Go.} game.  We will focus on the case in which $\mathcal S$ and
$\mathcal X$ are finite, and will call the input set $\mathcal A$ for
{\em actions} (rather than $\mathcal X$).  The idea is that an agent
(a robot or a game-player) can model its environment as an {\sc mdp}
and try to choose actions that will drive the process into states that
have high scores.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Definition and policies}
\label{sec_mdps}

Formally, a Markov decision process is $\langle \mathcal S, \mathcal A, T, R,
\gamma\rangle$ where:
\begin{itemize}
\item
$T : \mathcal S \times \mathcal A \times \mathcal S \rightarrow \R$ is
a {\em transition model}\index{transition model}, where
\[T(s, a, s') = P(S_t = s'|S_{t - 1} = s, 
A_{t - 1} = a)\;\;,\] specifying a conditional probability distribution; 
\note{The notation here uses capital letters, like $S$, to stand for
  random variables  and small letters to stand for concrete values.
  So $S_t$ here is a random variable that can take on elements  of
  $\mathcal S$ as values.}
\item
$R: \mathcal S \times \mathcal A \rightarrow \R$ is a reward function\index{reward function},
where $R(s, a)$ specifies how desirable it is to be in  state $s$ and
take action $a$; and  
\item $\gamma \in [0, 1]$ is a {\em discount factor}, which we'll
  discuss in Section~\ref{sec:discount}.
\end{itemize}

\begin{examplebox}
The following description of a simple machine as Markov decision
process provides a concrete example of an MDP.  
%
% An agent controls the actions taken, while the environment responds
% with the transition to the next state. 
%
The machine has three possible operations ({\em actions}): ``wash'',
``paint'', and ``eject'' (each with a corresponding button). Objects
are put into the machine. Each time you push a button, something is
done to the object. However, it's an old machine, so it's not very
reliable. The machine has a camera inside that can clearly detect what
is going on with the object and will output the state of the object:
``dirty'', ``clean'', ``painted'', or ``ejected''.  For each action,
this is what is done to the object:

\noindent {\bf Wash}:

\begin{itemize}
\item If you perform the ``wash'' operation on any object, whether
  it's dirty, clean, or painted, it will end up ``clean'' with
  probability 0.9 and ``dirty'' otherwise.
\end{itemize}

\noindent {\bf Paint}:

\begin{itemize}
\item If you perform the ``paint'' operation on a clean object, it
  will become nicely ``painted'' with probability 0.8. With
  probability 0.1, the paint misses but the object stays clean, and
  also with probability 0.1, the machine dumps rusty dust all over the
  object and it becomes ``dirty''.
\item If you perform the ``paint'' operation on a ``painted'' object,
  it stays ``painted'' with probability 1.0.
\item If you perform the ``paint'' operation on a ``dirty'' part, it
  stays ``dirty'' with probability 1.0.
\end{itemize}

\noindent {\bf Eject}:

\begin{itemize}
\item If you perform an ``eject'' operation on any part, the part
  comes out of the machine and this fun game is over. The part remains
  "ejected" regardless of any further action.
\end{itemize}

\noindent
These descriptions specify the transition model $T$, and the
transition function for each action can be depicted as a state machine
diagram.  For example, here is the diagram for ``wash'':
%
% As for the state machine diagrams presented above, states are denoted
% by large circles and actions by small black circles. The arc from a
% state to an action is followed by the possible arcs (from the small
% action circle) showing the probability of transition from the source
% state to the indicated destination state.
%
\begin{center}
\begin{tikzpicture}[->]
\node[state] (dirty) {dirty};
\node[state, right of=dirty, xshift=2.5cm] (clean) {clean};
\node[state, below of=dirty, yshift=-2.5cm] (painted) {painted};
\node[state, below of=clean, yshift=-2.5cm] (ejected) {ejected};
\draw (dirty) edge[loop above] node{0.1} (dirty)
      (dirty) edge[bend left, above] node{0.9} (clean)
      (clean) edge[loop above] node{0.9} (clean)
      (clean) edge[bend left, below] node{0.1} (dirty)
      (painted) edge[left] node{0.1} (dirty)
      (painted) edge[right] node{~0.9} (clean)
      (ejected) edge[loop above] node{1.0} (ejected);
\end{tikzpicture}
\end{center}

You get reward +10 for ejecting a painted object, reward 0 for
ejecting a non-painted object, reward 0 for any action on an "ejected"
object, and reward -3 otherwise.  The MDP description would be
completed by also specifying a discount factor.

\end{examplebox}



A {\it{policy}}\index{policy} is a function $\pi: \mathcal S \rightarrow \mathcal A$
that specifies what action to take in each state.  The policy is what
we will want to learn; it is akin to the strategy which a player
employs to win a given game.  Below, take just the initial steps
towards this eventual goal.  We describe how to evaluate how good a
policy is, first in the {\em finite horizon}  case
(Section~\ref{sec:mdp_finite_horizon}) when the total number of
transition steps is finite.  Then we consider the {\em infinite horizon} 
case (Section~\ref{sec:mdp_infinite_horizon}), when you
don't know when the game will be over.

There are important algorithms which can be used to find good
policies, under certain assumptions.  These will be be discussed in
Chapter~\ref{chap:reinforcement}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Finite-horizon policies}
\index{policy!finite horizon}
\label{sec:mdp_finite_horizon}

The goal of a policy is to maximize the total reward, averaged over
the stochastic transitions that the domain makes.  Let's first
consider the case where there is a finite {\em horizon} $H$,
indicating the total number of steps of interaction that the agent
will have with the {\sc mdp}.  And let's seek to measure the goodness of a
policy.

We do so by defining for a given {\sc mdp} policy $\pi$ and horizon
$h$, the ``horizon $h$ {\em value}'' of a state, $V^{h}_\pi(s)$.  We
do this by induction on the horizon, which is the {\em number of steps
  left to go}.

The base case is when there are no steps remaining, in which case, no
matter what state we're in, the value is 0,  so
\begin{equation*}
V^0_{\pi}(s) = 0\;\;.
\end{equation*}
Then, the value of a policy in state $s$ at horizon $h + 1$ is equal
to the reward it will get in state $s$ plus the next state's expected horizon $h$
value.  So, starting with horizons 1 and 2, and then
moving to the general case, we have:
\begin{align}
V^1_{\pi}(s) &= R(s, \pi(s)) + 0\\
V^2_{\pi}(s) &= R(s, \pi(s)) + \sum_{s'}T(s, \pi(s), s') \cdot R(s', \pi(s'))\\
\vdots\\
V^h_{\pi}(s) &= R(s, \pi(s)) + \sum_{s'}T(s, \pi(s), s') \cdot V^{h - 1}_{\pi}(s')
\label{eq:finite_value}
\end{align}
The sum over $s'$ is an {\em expected value}\index{expected value}:  it considers all
possible next states $s'$, and computes an average of their
$(h-1)$-horizon values, weighted by the probability that the transition
function from state $s$ with the action chosen by the policy,
$\pi(s)$, assigns to arriving in state $s'$.
\question{What is $\sum_{s'} T(s, a, s')$ for any particular $s$  and  $a$?
}

Then we can say that a policy $\pi_1$ is better than policy $\pi_2$ for horizon
$h$, i.e. $\pi_1 >_h \pi_2$, if and only if for all $s \in \mathcal S$,
$V_{\pi_1}^h(s) \geq V_{\pi_2}^h(s)$ and there exists at least one $s
\in \mathcal S$ such that $V_{\pi_1}^h(s) > V_{\pi_2}^h(s)$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Infinite-horizon policies}
\label{sec:mdp_infinite_horizon}
\label{sec:discount}
\index{policy!infinite horizon}

More typically, the actual finite horizon is not known, i.e. when you
don't know when the game will be over!  This is called the {\em
  infinite horizon} version of the problem.  How does one evaluate the
goodness of a policy in the infinite horizon case?

If we tried to simply take our definitions above and set $h = \infty$, 
we could get in trouble, because it could well be that the
$V^\infty$ values for all actions would be infinite, and there would
be no way to select one over the other.

There are two standard ways to deal with this problem.  One is to take
a kind of {\em average} over all time steps, but this can be a little
bit  tricky to think about.  We'll take a  different  approach,  which
is to  consider the {\em discounted} infinite horizon\index{policy!infinite horizon!discount factor}.   We select a 
discount factor $0 < \gamma < 1$. Instead of valuing a policy
based on an expected finite-horizon undiscounted value: 
\begin{equation}
 \mathbb{E}\left[\sum_{t = 0}^{h}R_t \mid \pi, s_0\right]\;\;, 
\end{equation}
where $R_t$ is the reward received at time $t$, we will evaluate the policy based on its expected {\it{infinite horizon
    discounted value}}, 
which is 
\begin{equation}
 \mathbb{E}\left[\sum_{t = 0}^{\infty}\gamma^tR_t \mid \pi, S_0\right] 
   = \mathbb{E}\left[R_0  + \gamma R_1 + \gamma^2 R_2 + \ldots \mid \pi,  s_0\right] \;\;.
\end{equation}
Note that the $t$ indices here are not the number of steps to go, but
actually the number of steps forward from the starting state (there is
no sensible notion  of ``steps to go'' in the infinite horizon case).

\begin{examplebox}
What is $\mathbb{E}\left[ \cdot \right ]$?  This mathematical notation
  indicates an {\em expectation value}, i.e. an average taken over all
  the random possibilities which may occur for the argument.  Here,
  $R_t \mid \pi, S_0$ also denotes a {\em conditional probability},
  where $R_t$ is the random value subject to the policy being $\pi$
  and the state being $s_0$.
%
~\\[1.5ex]
%
  In the above equations, $\mathbb{E}\left[ \cdot \right ]$ is being
  used as shorthand notation to indicate a concept.  The main
  objective is to get to Eq.~\ref{eq:inifite_horiz_value}, which can
  also be obtained essentially by adding $\gamma$ to
  Eq.~\ref{eq:finite_value}, with the appropriate definition of the infinite-horizon value.
\end{examplebox}

There are two good intuitive motivations for discounting.  One is
related to economic theory and the present value of money:   you'd
generally  rather have some money today than that same amount of money
next week (because you could use it now or invest it).  The other is
to think of the whole process  terminating, with probability
$1-\gamma$  on  each step of the interaction.    This  value  is the
expected amount of reward the  agent  would gain under  this
terminating model.

Let us now evaluate a policy in terms of the expected discounted
infinite-horizon value that the agent will get in the {\sc mdp} if it
executes that policy.  We define the infinite-horizon value of a state
$s$ under policy $\pi$ as
\begin{equation}
 V_{\pi}(s) = \mathbb{E}[R_0 + \gamma R_1 + \gamma^2 R_2 +
\dots \mid \pi, S_0 = s] = \mathbb{E}[R_0 + \gamma(R_1 + \gamma(R_2 + \gamma
\dots))) \mid \pi, S_0 = s] \;\;.
\end{equation}
Because the expectation of a linear combination of random variables is
the linear combination of the expectations, we have
\begin{align}
V_{\pi}(s) & = \mathbb{E}[R_0 \mid \pi, S_0 = s] + \gamma  \mathbb{E}[
R_1 + \gamma(R_2 + \gamma \dots))) \mid \pi, S_0 = s] 
\nonumber
\\
         & = R(s, \pi(s)) + \gamma\sum_{s'}T(s, \pi(s), s')V_{\pi}(s')
\label{eq:inifite_horiz_value}
\end{align}
\note{This is {\em so} cool!  In a discounted model, if you
  find that you survived this round  and landed  in some state $s'$,
  then you have the same expected future lifetime as you did before.
  So the value function\index{value function} that is relevant in that state is exactly the
  same one as  in state $s$.}

You could write down one of these equations for each of the $n =
|\mathcal  S|$ states. There are $n$ unknowns $V_{\pi}(s)$.  These
are linear equations,  and  so it's  easy  to solve them using
Gaussian elimination to find the value of each state under this
policy.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Finding policies for MDPs}

\label{sec:finding_mdp_policies}

Given an {\sc mdp}, our goal is typically to find a policy that is
optimal in the sense that it gets as much total reward as possible, in
expectation over the stochastic transitions that the domain makes.  We
build on what we have learned about evaluating the goodness of a
policy (Sections~\ref{sec:mdp_finite_horizon}
and~\ref{sec:mdp_infinite_horizon}), and find optimal policies for the
finite horizon case (Section~\ref{sec:mdp_finite_horizon_optimal}),
then the infinite horizon case
(Section~\ref{sec:mdp_infinite_horizon_optimal}).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Finding optimal finite-horizon policies}

\label{sec:mdp_finite_horizon_optimal}

How can we go about finding an optimal policy for an {\sc mdp}? \index{policy!optimal policy} We
could imagine enumerating all possible policies and calculating their
value functions\index{value function!calculating value function} as in the previous chapter and picking the best
one...but that's too much work!

The first observation to make is that, in a finite-horizon problem,
the best action to take depends on the current state, but also on the
horizon:  imagine that you are in a situation where you could reach a
state with reward 5 in one step or a state with reward 10 in two
steps.  If you have at least two steps to go, then you'd move toward
the reward 10 state, but if you only have step left to go, you should
go in the direction that  will  allow you to gain 5!

One way to find an optimal policy is to compute an {\em  optimal
  action-value function}, $Q$\index{Q-function}\index{optimal action-value function}.  For the finite-horizon case, we define 
$Q^h(s, a)$ to be the expected value of 
\begin{itemize}
\item starting in state $s$,
\item executing action $a$, and
\item continuing for $h - 1$ more steps executing an optimal policy
  for the appropriate horizon on each step.
\end{itemize}
Similar to our definition of $V^h$ for evaluating a policy, we define
the $Q^h$ function recursively according to the horizon.  The only
difference is that, on each step with horizon $h$, rather than
selecting an action specified by a given  policy, we select the value
of $a$ that will maximize the expected $Q^h$ value of the next state.
\begin{align}
Q^0(s, a) &= 0\\
Q^1(s, a) &= R(s, a) + 0\\
Q^2(s, a) &= R(s, a) + \sum_{s'}T(s, a, s') \max_{a'} R(s', a')\\
\vdots
\nonumber \\
Q^h(s, a) &= R(s, a) + \sum_{s'}T(s, a, s') \max_{a'} Q^{h - 1}(s', a')
\end{align}
We can solve for the values of $Q^h$ with a simple recursive algorithm
called {\it{finite-horizon value iteration}} which just computes $Q^h$ starting
from horizon 0 and working backward to the desired horizon
$H$. Given $Q^h$, an optimal finite-horizon policy $\pi_h^*$ is easy to find: 
\begin{equation}
 \pi_h^*(s) = \text{arg}\max_{a}Q^h(s, a) \;\;.
\end{equation}
There may be multiple possible optimal policies.

\begin{examplebox} {\bf Dynamic programming}
  (somewhat
counter-intuitively, dynamic programming is neither really ``dynamic''
nor a type of ``programming'' as we typically understand it) is
a technique for designing efficient algorithms.  Most methods for
solving MDPs or computing value functions rely on dynamic
programming to be efficient.

  The {\em principle of dynamic programming}\index{dynamic programming} is to compute and store the
solutions to simple sub-problems that can be re-used later in the
computation.  It is a very important tool in our algorithmic toolbox.

  Let's consider what would happen if we tried to compute $Q^4(s,
  a)$ for all $(s, a)$ by directly using the definition:
  \begin{itemize}
  \item To compute $Q^4(s_i, a_j)$ for any one $(s_i, a_j)$, we would
    need to compute $Q^3(s, a)$ for all $(s, a)$ 
    pairs.
  \item To compute $Q^3(s_i, a_j)$ for any one $(s_i, a_j)$, we'd need to
    compute $Q^2(s, a)$ for all $(s, a)$ pairs.
  \item To compute $Q^2(s_i, a_j)$ for any one $(s_i, a_j)$, we'd
    need to compute $Q^1(s, a)$ for all $(s, a)$ pairs.
  \item Luckily, those are just our $R(s, a)$ values.
  \end{itemize}

So, if we have $n$ states and $m$ actions, this is $O((mn)^3)$
work---that seems like way too much, especially as the horizon
increases!  But observe that we really only have $mnh$ values that
need to be computed, $Q^h(s, a)$ for all $h, s, a$.  If we start with
$h=1$, compute and store those values, then using and reusing the
$Q^{h-1}(s, a)$ values to compute the $Q^h(s, a)$ values, we can do
all this computation in time $O(mnh)$, which is much better!

\end{examplebox}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Finding optimal infinite-horizon policies}
\label{sec:mdp_infinite_horizon_optimal}

In contrast to the finite-horizon case, the best way of behaving in an
infinite-horizon discounted {\sc mdp} is not time-dependent: at every
step, your expected future lifetime, given that you have survived
until now, is $1 / (1 - \gamma)$.
%
\question{
Verify this fact:  if, on every day you wake up, there is a
probability of $1 - \gamma$ that today will be your last day, then your
expected lifetime is $1 /  (1 - \gamma)$ days.
}

An important theorem about {\sc mdp}s is: in the infinite-horizon case, there exists a stationary\index{policy!stationary policy}
\note{Stationary means that it doesn't change over time; in contrast, the
  optimal policy in a finite-horizon {\sc mdp} is {\em non-stationary.}}
optimal policy $\pi^*$ (there may be more than one) such that for all
$s \in \mathcal S$ and all other policies $\pi$, we have 
\begin{equation}
 V_{\pi^*}(s) \ge V_{\pi}(s) \;\;.
\end{equation}

% Let $n = |\mathcal S|$ andd $m = |\mathcal A|$. Algorithm for finding $\pi^*$:
% \begin{itemize}
% \item
% enumerate and test, complexity is $O(m^n)$
% \item
% linear programming, complexity is $O(\text{poly}(n, m b))$ where $b$ is the number of bits per element of $T, R$
% \item
% policy iteration, complexity is $O(\text{poly}(n, m, \text{bits}(\gamma)))$, requires solving lots of $n \times n$ systems
% \item
% {\underline{value iteration}}, complexity is $O\left(\text{poly}(n, m, b, \frac{1}{1 - \gamma}\right)$
% \end{itemize}
% The latter is easy to implement and the foundation for many reinforcement-learning methods.

There are many methods for finding an optimal policy for an {\sc mdp}.
We have already seen the finite-horizon value iteration case.  Here we
will study a very popular and useful method for the infinite-horizon
case, {\em infinite-horizon value iteration}.  It is also important to
us, because it is the basis of many {\em reinforcement-learning}
methods.

Define $Q^*(s, a)$ to be the expected infinite-horizon discounted
value of being in state $s$, executing action $a$, and executing
an optimal  policy $\pi^*$ thereafter.  Using similar reasoning to the
recursive definition  of  $V_\pi$,  we can express this value
recursively as
\begin{equation}
 Q^*(s, a) = R(s, a) + \gamma\sum_{s'}T(s, a, s')\max_{a'}Q^*(s',
a') \;\;.
\end{equation} 
This is also a set of equations, one for each $(s, a)$ pair.  This
time, though, they are not linear, and so they are not easy to solve.
But there is a theorem  that says they have a unique solution!

If we knew the optimal action-value function, then  we could derive an
optimal policy  $\pi^*$ as
\begin{equation}
 \pi^*(s) = \text{arg}\max_{a}Q^*(s, a) \;\;.
\end{equation}
\question{The optimal value function is unique, but the optimal policy
  is not.  Think of a situation in which there is more than one
  optimal policy.}

We can iteratively solve for the $Q^*$ values with the infinite-horizon
value iteration algorithm, shown below:

\begin{codebox}
  \Procname{$\proc{Infinite-Horizon-Value-Iteration}(\mathcal S, \mathcal A, T, R, \gamma, \epsilon)$}
  \li     \For $s \in \mathcal{S}, a \in \mathcal{A}:$
	\Do
  \li        $Q_{\text{old}}(s, a) = 0$
        \End
  \li     \While True:
        \Do
  \li        \For $s \in \mathcal{S}, a \in \mathcal{A}:$
           \Do
  \li           $Q_{\text{new}}(s, a) = R(s, a) + \gamma\sum_{s'}T(s, a, s')\max_{a'}Q_{\text{old}}(s', a')$
         \End

  \li      \If $\max_{s, a}\lvert Q_{\text{old}}(s, a) - Q_{\text{new}}(s, a)\rvert < \epsilon:$
         \Do
  \li           return $Q_{\text{new}}$
        \End
  \li      $Q_{\text{old}} = Q_{\text{new}}$
	\End
\end{codebox}

\paragraph*{Theory}

There are a lot of nice theoretical results about infinite-horizon value iteration.
For some given (not necessarily optimal) $Q$ function, define
$\pi_{Q}(s) = \text{arg}\max_{a}Q(s, a)$.   
\begin{itemize}
\item After executing infinite-horizon value
iteration with parameter $\epsilon$,  
\note{Note the new 
  notation!  Given two functions $f$ and $f'$, we
  write $\lVert f - f' \rVert_\text{max}$ to mean $\max_x \lvert f(x)
  - f'(x)\rvert$.   It measures the maximum absolute
  disagreement between the two functions at any input $x$.}
\begin{equation}
\lVert  V_{\pi_{Q_{\text{new}}}} - V_{\pi^*} \rVert_{\text{max}} < \epsilon \;\; .
\end{equation}
%
\item
There is a value of $\epsilon$ such that
\begin{equation}
 \Vert Q_{\text{old}} - Q_{\text{new}} \rVert_{\text{max}} <
\epsilon \Longrightarrow \pi_{Q_{\text{new}}} = \pi^* 
\end{equation}  
\item  As the algorithm executes,   
$\lVert V_{\pi_{Q_{\text{new}}}} - V_{\pi^*} \Vert_{\text{max}}$ decreases
monotonically on each iteration.
\item The algorithm  can be executed asynchronously, in parallel: as
  long as all $(s, a)$ pairs are updated infinitely often in an
  infinite run, it still converges to optimal value. 
\note{This is very important for reinforcement learning.}

\end{itemize}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "top"
%%% End:
