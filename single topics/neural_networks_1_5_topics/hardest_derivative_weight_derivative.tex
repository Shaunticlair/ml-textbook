
    \subsection*{7.X.20 \quad The weight derivative}
        
        \begin{equation}
            \overbrace{
                \pderiv {\red{Z^\ell}}   {\blu{W^{\ell}}} 
            }^{ (m^{\ell} \times 1) ?}
        \end{equation}
        
        This derivative is difficult - it's a derivative in the form vector/matrix. With \textbf{three} axes, we might imagine representing as a 3-tensor.
        
        In fact, this can be manipulated into multiple different interesting \textbf{shapes} based on your \textbf{interpretation}: as we mentioned, there's no consistent rule for these variables.
            
        But, our goal is to use this for the \textbf{chain rule}: so, we need to make the shapes \textbf{match}. This is why we do that strange transposing for our complete derivative.
        
            \begin{equation}
                \pderiv {\pur{\loss}} {\blu{W^{\ell}}} 
                =
                \overbrace{
                    \pderiv {\red{Z^\ell}}   {\blu{W^{\ell}}}
                }^{\text{Weight link}} 
                    \cmul
                \overbrace{
                    \Bigg(
                        \pderiv {\pur{\loss}} {\red{Z^\ell}}
                    \Bigg)^T
                }^{\text{Other layers}}
            \end{equation}
            
        \subsecdiv
        
        Our problem is we have \textbf{too many axes}: the easiest way to resolve this to \textbf{break up} our matrix. So, for now, we focus on only \textbf{one neuron} at a time: it has a column vector $\blu{W_i}$.
            \note{For simplicity, we're gonna ignore the $\ell$ notation: just be careful, because $Z$ and $A$ are from two different layers!}
        
        \begin{equation}
            \blu{W}
            =
            \begin{bmatrix}
                \blu{W}_1 & \blu{W}_2 & \cdots & \blu{W}_n
            \end{bmatrix}
        \end{equation}
        
        Notice that, this time, we broke it into \textbf{column vectors}, rather than row vectors: each neuron's \textbf{weights} are represented by a column vector.
        
        We'll ignore everything except $\blu{W_i}$.
        
        \begin{equation}
            \blu{W_i} = 
            \begin{bmatrix}
                \blu{w}_{1i} \\ \blu{w}_{2i} \\ \vdots \\ \blu{w}_{mi}
            \end{bmatrix}
        \end{equation}
        
        Finally, we get into our equation: notice that a \textbf{single} neuron has only \textbf{one} pre-activation $z_i$, so we don't need the whole vector.
        
        \begin{equation}
            \red{z_i} = \blu{W_i^T}\org{A}
        \end{equation}
        
        Wait: there's something to notice, right off the bat. $\red{z_i}$ is \textbf{only} a function of $\blu{W_i}$: that means the derivative for every other term $\pderivslash{}{\blu{W_k}}$ is \textbf{zero}!
            \note{For example, changing $\blu{W_2}$ would have \textbf{no} effect on $\red{z_1}$.}\\
        
        \begin{concept}
            The $\nth{i}$ neuron's \vocab{weights}, $W_i$, have \gren{no effect} on a different neuron's \purp{pre-activation} $z_j$.
            
            So, if the \purp{neurons} don't match, then our derivative is zero:
            
            \begin{itemize}
                \item $i$ is the neuron for pre-activation $\red{z_i}$
                \item $j$ is the $\nth{j}$ \blu{weight} in a neuron.
                \item $k$ is the neuron for weight vector $\blu{W_k}$
            \end{itemize}
            
            \begin{equation*}
                \pderiv{ \red{z}_i }{ \blu{W}_{jk} } = 0 
                \qquad
                \text{if } i \neq k
            \end{equation*}
            
            So, our only nonzero derivatives are
            
            \begin{equation*}
                \pderiv{ \red{z}_i }{ \blu{W}_{ji} }
            \end{equation*}
        \end{concept}
        
        \subsecdiv
        
        With that done, let's substitute in our values:
        
        \begin{equation}
            \red{z_i} 
            =
            \begin{bmatrix}
                \blu{w}_{1i} & \blu{w}_{2i} & \cdots & \blu{w}_{mi}
            \end{bmatrix}
            \begin{bmatrix}
                \org{a_1}\\\org{a_2}\\ \vdots \\ \org{a_m}
            \end{bmatrix}
        \end{equation}
        
        And we'll do our \textbf{matrix multiplication}:
        
        \begin{equation}
            \red{z_i} 
            =
            \sum_{j=1}^n
            \blu{W_{ji} }\org{a_j}
        \end{equation}
        
        Finally, we can get our derivatives:
        
        \begin{equation}
            \pderiv{ \red{z_i} }{  \blu{W_{ji} }} = \org{a_j}
        \end{equation}
        
        So, if we combine that into a vector, we get:
        
        \begin{equation}
            \pderiv{ \red{z_i} }{  \blu{W_{i} }} =
            \begin{bmatrix}
                \bigpderiv{ \red{z_i} }{  \blu{W_{1i} }}
                \\\\
                \bigpderiv{ \red{z_i} }{  \blu{W_{2i} }}
                \\\\
                \vdots
                \\\\
                \bigpderiv{ \red{z_i} }{  \blu{W_{mi} }}
            \end{bmatrix}
        \end{equation}
        
        We can use our equation:
        
        \begin{equation}
            \pderiv{ \red{z_i} }{  \blu{W_{i}} } =
            \begin{bmatrix}
                \org{a_1}
                \\
                \org{a_2}
                \\
                \vdots
                \\
                \org{a_m}
            \end{bmatrix}
            =
            \org{A}
        \end{equation}
        
        We get a result!
        
        \subsecdiv
        
        What if the pre-activation $\red{z_i}$ and weights $\blu{W_{k}}$ don't match? We've already seen: the derivative is 0: weights don't affect different neurons.
        
        \begin{equation}
            \pderiv{ \red{z}_i }{ \blu{W}_{jk} } = 0 
            \qquad
            \text{if } i \neq k
        \end{equation}
        
        We can combine these into a \textbf{zero vector}:
        
        \begin{equation}
            \pderiv{ \red{z}_i }{ \blu{W}_{k} } = 
            \begin{bmatrix}
                0\\0\\ \vdots \\ 0
            \end{bmatrix}
            =
            \vec{0}
            \qquad
            \text{if } i \neq k
        \end{equation}
        
        So, now, we can describe all of our vector components:
        
        \begin{equation}
            \pderiv{ \red{z}_i }{ \blu{W}_{k} } = 
            \begin{cases}
              \org{A} & \text{if } i = k\\
              \vec{0} & \text{if } i \neq k
            \end{cases}
        \end{equation}
        
        These are all the elements of our matrix $\pderivslash{ \red{z}_i }{ \blu{W}_{k} }$: so, we can get our result.
        
        \begin{equation}
            \pderiv{ \red{Z} }{ \blu{W} } 
            =
            \begin{bmatrix}
                \org{A} & \vec{0} & \cdots  & \vec{0} \\
                \vec{0} & \org{A} & \cdots  & \vec{0} \\
                \vdots  & \vdots  & \ddots  & \vec{0} \\
                \vec{0} & \vec{0} & \vec{0} & \org{A}
            \end{bmatrix}
        \end{equation}
        
        We have our result: it turns out, despite being stored in a \textbf{matrix}-like format, this is actually a \textbf{3-tensor}! Each entry of our \textbf{matrix} is a \textbf{vector}: 3 axes.
        
        \subsecdiv
        
        But, we don't really... \textit{want} a tensor. It doesn't have the right shape, and we can't do matrix multiplication.
        
        We'll solve this by \textbf{simplifying}, without losing key information.\\

        \begin{concept}
            For many of our "tensors" resulting from matrix derivatives, they contain \purp{empty} rows or \gren{redundant} information. 
            
            Based on this, we can \vocab{simplify} our tensor into a fewer-dimensional (fewer axes) object.
        \end{concept}
        
        We can see two types of \textbf{redundancy} above:
        
        \begin{itemize}
            \item Every element \textbf{off} the diagonal is 0.
            \item Every element \textbf{on} the diagonal is the same.
        \end{itemize}
        
        Let's fix the first one: we'll go from a diagonal matrix to a column vector.
        
        \begin{equation}
            \begin{bmatrix}
                \org{A} & \vec{0} & \cdots  & \vec{0} \\
                \vec{0} & \org{A} & \cdots  & \vec{0} \\
                \vdots  & \vdots  & \ddots  & \vec{0} \\
                \vec{0} & \vec{0} & \vec{0} & \org{A}
            \end{bmatrix}
            \longrightarrow
            \begin{bmatrix}
                \org{A} \\ \org{A} \\ \vdots \\ \org{A} 
            \end{bmatrix}
        \end{equation}
        
        Then, we'll combine all of our redundant $A$ values.
        
        \begin{equation}
            \begin{bmatrix}
                \org{A} \\ \org{A} \\ \vdots \\ \org{A} 
            \end{bmatrix}
            \longrightarrow
            \org{A}
        \end{equation}
        
        We have our big result!
        
        \begin{notation}
            Our derivative
            
            \begin{equation*}
                \overbrace{
                    \pderiv {\red{Z^\ell}}   { \blu{W^{\ell}} } 
                }^{ (m^{\ell} \times 1) }
                = 
                \org{A^{\ell-1}}
            \end{equation*}
        
            Is a vector/matrix derivative, and thus should be a 3-tensor.
            
            But, we have turned it into the shape \blu{$(m^{\ell} \times 1)$}.
        \end{notation}
        
        This is as \textbf{condensed} as we can get our information: if we compress to a scalar, we lose some of our elements.
        
        Even with this derivative, we still have to do some clever \textbf{reshaping} to get the result we need (transposing, changing derivative order, etc.)
        
        However, at the end, we get the right shape for our chain rule!
        