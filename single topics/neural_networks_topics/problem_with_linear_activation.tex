\section*{Choices of activation function}

    Our linear model is entirely \textbf{defined} by its input: the number of \textbf{weights} in a neuron is just the number of \textbf{inputs} $m$.
    
    But our \textbf{activation} function is up to us to decide: what works best?
    
    \subsection*{Trying out linear activation}
        
        The simplest assumption would be to just use the \textbf{identity} function 
        
        \begin{equation}
            f(z) = z
        \end{equation}
        
        We might hope that we can combine a bunch of simple, \textbf{linear} models, and get a more sophisticated model. Why bother having a \textbf{nonlinear} activation at all?
        
        Well, it turns out, combining \textbf{multiple} linear layers doesn't make our model any stronger. Let's try an example: we'll take a network with 2 layers, two neurons each.
        
        Let's look at layer 1: 
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=70mm,scale=0.4]{images/nn_images/layer_one_linear.png}
        \end{figure}
        
        Since the activation function has \textbf{no effect} on our result, we can \textbf{omit} it:
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=70mm,scale=0.4]{images/nn_images/linear_layer_omitted.png}
        \end{figure}
        
        And now, we can show our \textbf{full} network:
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=70mm,scale=0.4]{images/nn_images/two_layers_linear.png}
        \end{figure}
        
    \subsection*{Linear Layers: An example}
        
        We'll assume \textbf{two} inputs $A_0 = [x_1, x_2]^T$. For our sanity, we'll lump all of the weights in each \textbf{layer}:
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=70mm,scale=0.4]{images/nn_images/two_layer_linear_layer_view.png}
        \end{figure}
        
        We'll leave out $W_0$ terms to make it more readable, but the same will apply. 
        
        Layer 1:
        
        \begin{equation}
            A^1 = (\red{W^1})^T \pur{A_0} 
        \end{equation}
        
        Layer 2:
        
        \begin{equation}
            A^2 = 
            \overbrace{
            (\blu{W^2})^T (\red{W^1})^T
            }^{\text{Weight matrices}}
            \pur{A_0} 
        \end{equation}
        
        The full function for this equation is two matrices, \textbf{multiplied} by our input vector.
        
        Let's take an arbitrary example:
        
        \begin{equation}
            W^1 = 
            \red{
            \begin{bmatrix}
                1 & 2 \\ 3 & 4
            \end{bmatrix}
            }
            \qquad
            \blu{
            W^2 = 
            \begin{bmatrix}
                5 & 6 \\ 7 & 8
            \end{bmatrix}
            }
        \end{equation}
        
        Our equation becomes:
        
        \begin{equation}
            A^2 = 
            \overbrace{
                \blu{
                \begin{bmatrix}
                    5 & 7 \\ 6 & 8
                \end{bmatrix}
                }
                \red{
                \begin{bmatrix}
                    1 & 3 \\ 2 & 4
                \end{bmatrix}
                }
            }^{\text{Transposed matrices}}
            \pur{
            \begin{bmatrix}
                x_1 \\ x_2
            \end{bmatrix}
            }
        \end{equation}
        
        We created this function by applying two matrices separately. But, can't we \textbf{combine} them?
       
        \begin{equation}
            A^2 =
            \begin{bmatrix}
                19 & 43 \\ 22 & 50
            \end{bmatrix}
            \pur{
            \begin{bmatrix}
                x_1 \\ x_2
            \end{bmatrix}
            }
        \end{equation}
       
        Wait, but this looks like a \textbf{one-layer} network with those weights! The second layer is \textbf{pointless}, we could have represented it with a single layer...
       
        \begin{equation}
           (W^{12})^T = 
           \begin{bmatrix}
                19 & 43 \\ 22 & 50
            \end{bmatrix}
        \end{equation}
    
    \subsection*{The problem with linear networks}
    
        In fact, this is true in general: we can always take our \textbf{two} linear layers and combine them into \textbf{one}.
        
        \begin{equation}
            (\blu{W^2})^T (\red{W^1})^T
            = W^{12}
        \end{equation}
        
        Our network is \textbf{equivalent} to the supposedly "simpler" one-layer network.
    
        \begin{figure}[H]
            \centering
            \includegraphics[width=80mm,scale=0.4]{images/nn_images/equivalent_networks.png}
        \end{figure}
        
        What if we have more layers? Well, we can just combine them one-by-one. At the end, we're just left with one layer:
        
        \begin{equation}
            (\pur{W^L})^T (\org{ {W^{L-1}} })^T \cdots (\blu{W^2})^T (\red{W^1})^T
            = W
        \end{equation}
        
        And so, we can't just use linear layers: we \textbf{need} a \textbf{nonlinear} activation function.\\
        
        \begin{concept}
            Having multiple consecutive \vocab{linear layers} (i.e. layers with linear \gren{activation} functions) is \purp{equivalent} to having one linear layer in its place.
            
            This means that we do not expand our \purp{hypothesis} class by using more linear layers: we have to use \gren{nonlinear} activation functions.
        \end{concept}

        This problem is even worse than it seems: let's see why. Since we can \textbf{combine} $n$ linear layers together into one, what happens if we only have \textbf{one} linear layer?

        Suppose layer $\ell$ is linear. The next layer contains a \textbf{linear} component and a non-linear \textbf{activation} component. We'll focus on just the linear part.
            \note{Activation comes after this step, so we would just use $f(z^{\ell+1})$.}

        \begin{equation}
            z^{\ell+1} = (\blu{W^{\ell+1}})^T\red{x^{\ell+1}} = 
            (\blu{W^{\ell+1}})^T \red{ (W^{\ell})^T } \pur{ x^{\ell} } 
        \end{equation}

        Wait: we have \textbf{two} consecutive \textbf{linear} components. We can combine layer $\ell$ with the linear component of the next layer! 

        \begin{equation}
            (\blu{W^{\ell+1}})^T \red{ (W^{\ell})^T } \pur{ x^{\ell} }  = W \pur{ x^{\ell} }
        \end{equation}
        
        Now, we've removed layer $\ell$ entirely: it makes no difference to have even just one \textbf{hidden} linear layer!\\

        \begin{concept}
            Even having one hidden \vocab{linear layer} is \purp{redundant}: it's \purp{equivalent} to not having that layer at all.

            Since this requires \gren{more computation} for no benefit, we \purp{almost never} make linear hidden layers.
        \end{concept}

        So, linear models are out. What if we use something \textbf{nonlinear}?
        
        \begin{equation}
            A^2 = 
            \org{f}
            \Bigg( 
                (\blu{W^2})^T 
                \red{A^1}
            \Bigg)
        \end{equation}
        
        We get something that doesn't seem to \textbf{simplify}:
            \note{This is ugly, but we don't have to worry about the details.}
        
        \begin{equation}
            A^2 = 
            \org{f}
            \Bigg( 
                (\blu{W^2})^T 
                \overbrace{
                    \boxed{
                        \org{f}
                        \Big(
                            \red{ (W^1)^T x }
                        \Big)
                    }
                }^{A^1}
            \Bigg)
        \end{equation}

        If we choose our function right (and avoid linearity), this cannot be simplified to a single layer! That means, this function is \textbf{different} (and likely more \textbf{complex}) than a one-layer model. 
        
        And this kind of \textbf{expressiveness} is exactly what we're looking for.