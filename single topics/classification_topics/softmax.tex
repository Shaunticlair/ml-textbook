    \subsection*{Probabilities in multi-class}
    
        So, we now know our \textbf{problem}: we're taking in a data point $x \in \RR^d$, and \textbf{outputting} one of the classes as a \textbf{one-hot vector}.
        
        So, now that we know what sorts of data we're \textbf{expecting}, we need to decide on the formats of our \textbf{answer}.
        
        We'll be returning a vector of length-$k$: \textbf{one} for each \textbf{class}. When we were doing \textbf{binary} classification, we estimated the \textbf{probability} of the positive class.
        
        So, it should make sense to do the same \textbf{here}: for each class, we'll return the estimated \textbf{probability} of our data point being in that class.
        
        \begin{equation}
            g
            =
            \begin{bmatrix}
              \p{x \text{ in } C_1} \\ 
              \p{x \text{ in } C_2} \\ 
              \vdots \\ 
              \p{x \text{ in } C_k} 
            \end{bmatrix}
            =
            \begin{bmatrix}
              g_1\\g_2\\\vdots\\g_k
            \end{bmatrix}
        \end{equation}
        
        We need one \textbf{additional} rule: the probabilities need to add up to \textbf{one}: we should assume our point ends up in some class or \textbf{another}.
        
        \begin{equation}
            g_1+g_2+...+g_k=1
        \end{equation}
        
        \begin{concept}
            The different terms of our \vocab{multi-class} guess $g_i$ represent the \purp{probability} of our data point being in class $C_i$.
            
            Because we should assume our data point is in \purp{some} class, all of these probabilities have to \gren{add} to 1.
        \end{concept}

        Let's be careful, though: this is only true for probabilities within a single data point.
        
        \miniex Suppose you have two animals (data points).
        
            \begin{itemize}
                \item It's impossible for the first animal to be \textbf{both} 90\% cat and 90\% dog.
                \item \textit{But}, there's no issue with the first animal being 90\% cat and the second animal being 90\% dog.\\
            \end{itemize}

        \begin{clarification}
            It's only true that all of the probabilities for the \purp{same data point} need to add to 1.

            If you have $\p{\text{class 1}}$ for one data point and $\p{\text{class 2}}$ for another data point, those \gren{aren't related}.
        \end{clarification}
        
        So, we want to scale our values so they add to 1: this is called \textbf{normalization}. How do we do that?
        
        Well, let's say each class gets a \textbf{value} of $r_i$, before being \textbf{normalized}. For now, let's ignore how we got $r_i$, just know that we have it.
        
        To make the total 1, we'll \textbf{scale} our terms by a factor $C$:
        
        \begin{equation}
            C(r_1+r_2+...r_k) 
            = 
            C \left(\sum_{i=1}^k r_i \right)
            =
            1  
        \end{equation}
        
        We can get our factor $C$ just by dividing:
        
        \begin{equation}
            C 
            =
            \frac{1}{\sum r_i}
        \end{equation}
        
        We've got our desired $g_i$ now!
        
        \begin{equation}
            g = 
            \begin{bmatrix}
                r_1/\sum r_i  \\
                r_2/\sum r_i  \\
                \vdots  \\
                r_k/\sum r_i  \\
            \end{bmatrix}
        \end{equation}
        
    \subsection*{Turning sigmoid multi-class}
    
        Now, we just need to compute $r_i$ terms to plug in. To do that, we'll see how we did it using sigmoid:
        
        \begin{equation}
            g= \sigma(u) = \frac{1}{1+e^{-u}}
        \end{equation}
        
        This function is 0 to 1, which is good for being a probability. 
        
        Just for our convenience, we'll switch to positive exponents: all we have to do is multiply by $e^u/e^u$.
            \note{Negative numbers are easy to mess up in algebra.}
        
        \begin{equation}
            g = \frac{e^u}{e^u+1}
        \end{equation}
        
        We'll think of \textbf{binary} classification as a special case of \textbf{multi-class} classification. The above probability could be thought of as $g_1$: the chance of our first class.\\
        
        \begin{concept}
            \vocab{Binary classification} is a \purp{special} case of \vocab{multi-class} classification with only \gren{two} classes. 
            
            So, we can use it to figure out the \purp{general} case.
        \end{concept}
        
        So, what was our \textbf{second} probability, $1-g$? This will be our second class, $g_2$.
        
        \begin{equation}
            g_2 = 1-g= \frac{1}{1+e^{u}}
        \end{equation}
        
        This follows an $r_i/(\sum r_i)$ format: the numerators (1 and $e^u$) add to \textbf{equal} the denominator ($1+e^u$).
        
        \begin{equation}
            g = 
            \begin{bmatrix}
                1/(1+e^u)  \\
                e^u/(1+e^u)  \\
            \end{bmatrix}
        \end{equation}
        
        How do we \textbf{extend} this to \textbf{more} classes? Well, 1 and $e^u$ are \textbf{different} functions: this a problem. We want to be able to \textbf{generalize} to many $r_i$. 
        
        How do they make them \textbf{equivalent}? We could say $1=e^0$. So, we could treat both terms as \textbf{exponentials}!
        
        \begin{equation}
            g_1 = \frac{e^u}{e^0+e^u}
        \end{equation}
        
        We can do this for an \textbf{arbitrary} number of terms. We'll treat them as \textbf{exponentials}, just like for $e^u$ and $e^0$
        
        \begin{equation}
             g_i 
            =             \frac{r_i}{\sum r_j} 
            = 
            \frac{e^{u_i}}{\sum e^{u_j}}
        \end{equation}

        Now, we have a template for expanding into higher dimensions!
        
    \subsection*{Our Linear Classifiers}
    
        What are each of those $u_i$ terms? When we were doing \textbf{binary classification}, we used a \textbf{linear regression} function to help generate the probability:
            \note{Remember that $u(x)$ is not a probability yet: we used a sigmoid to turn it \textit{into} a probability.}
        
        \begin{equation}
            u(x) = \theta^Tx+\theta_0
        \end{equation}

        Now, we want multiple probabilities. So, we create multiple different functions $u_i$: $k$ different linear regression models $(\theta,\theta_0)$. We'll represent each vector as $\theta_{(i)}$.
        
        \begin{equation}
            \theta_{(1)} = 
            \begin{bmatrix}
                \theta_{1(1)} \\ \theta_{2(1)} \\ \vdots \\ \theta_{d(1)}
            \end{bmatrix}
            \qquad\qquad
            \theta_{(2)} = 
            \begin{bmatrix}
                \theta_{1(2)} \\ \theta_{2(2)} \\ \vdots \\ \theta_{d(2)}
            \end{bmatrix}
            \qquad\qquad
            \theta_{(k)} = 
            \begin{bmatrix}
                \theta_{1(k)} \\ \theta_{2(k)} \\ \vdots \\ \theta_{d(k)}
            \end{bmatrix}
        \end{equation}
        
        Each of these models could be seen as a "different perspective" of our data point: what about that data point is prioritized (large $\theta_i$ magnitudes), how do we bias the result ($\theta_0$)?
        
        This "perspective" we call $\theta_{(i)}$ will tell us if our data point is "closer" to the class it represents. And we compute the result with:
        
        \begin{equation}
            u_1(x) = \theta_{(1)}^T x + \theta_{0(1)}
            \qquad\qquad
            u_2(x) = \theta_{(2)}^T x + \theta_{0(2)}
            \quad \quad
            u_k(x) = \theta_{(k)}^T x + \theta_{0(k)}
        \end{equation}

        In the last section, we emphasized that we can only use $\sum p_i=1$ for the probabilities of a \textbf{single} data point. Based on this, we'll focus on only one data point.\\

        \begin{clarification}
            In this section, $x$ represents only \purp{one data point} $\ex{x}{i}$.

            Softmax treats each data point \gren{individually}, so it's easier to not group them together. 
        \end{clarification}
        
        Having all these separate equations for $\theta_i$ is tedious. Instead, we can combine them all into a $(d \times k)$ \textbf{matrix}.
            \note{$k$ classes, so we need $k$ classifiers. We'll stack them side-by-side like how we stacked multiple data points to create $X$.}
        
        \begin{equation}
            \theta = 
            \begin{bmatrix}
                \theta_{(1)} & \theta_{(2)} & \cdots & \theta_{(k)}
            \end{bmatrix}
            =
            \begin{bmatrix}
                \theta_{1(1)} & \theta_{1(2)} & \cdots      & \theta_{1(k)}\\ 
                \theta_{2(1)} & \theta_{2(2)} & \cdots      & \theta_{2(k)}\\ 
                \vdots      & \vdots      & \ddots      & \vdots     \\ 
                \theta_{d(1)} & \theta_{d(2)} & \cdots      & \theta_{d(k)}
            \end{bmatrix}
        \end{equation}
        
        And our constants, $\theta_0$, in a $(k \times 1)$ matrix:
        
        \begin{equation}
            \theta_0 =
            \begin{bmatrix}
                \theta_{0(1)} \\ \theta_{0(2)} \\ \vdots \\ \theta_{0(k)}
            \end{bmatrix}
        \end{equation}\\
        
        \begin{concept}
            We can combine \vocab{multiple classifiers} $\Theta_{(i)}=\Big(\theta_{(i)}, \theta_{0(i)} \Big)$ into large \textbf{matrices} $\theta$ and $\theta_0$ to compute \purp{multiple} outputs $u_i$ at the \gren{same} time.
        \end{concept}
        
        This will put all of our terms into a $(1 \times k)$ vector $u$.
        
        \begin{equation}
            u(x) = 
            \theta^T x + \theta_0
            =
            \begin{bmatrix}
                u_1 \\ u_2 \\ \vdots \\ u_k
            \end{bmatrix}
        \end{equation}
    
    \subsection*{Softmax}
        
        We now have all the pieces we need. Our \textbf{linear regression} for each class:
        
        \begin{equation}
            \begin{bmatrix}
                u_1 \\ u_2 \\ \vdots \\ u_k
            \end{bmatrix}
            = 
            \theta^T x + \theta_0
        \end{equation}
        
        The \textbf{exponential} terms, to get \textbf{logistic} behavior:
        
        \begin{equation}
            r_i = e^{u_i}
        \end{equation}
        
        The \textbf{averaging} to get probability = 1:
        
        \begin{equation}
            g = 
            \begin{bmatrix}
                r_1/\sum r_i  \\
                r_2/\sum r_i  \\
                \vdots  \\
                r_k/\sum r_i  \\
            \end{bmatrix}
        \end{equation}
        
        And so, our multiclass function is...\\
        
        \begin{definition}
            The \vocab{softmax function} allows us to calculate the probability of a point being in each class:
            
            \begin{equation*}
                g = 
                \begin{bmatrix}
                    e^{u_1}/\sum e^{u_i}  \\
                    e^{u_2}/\sum e^{u_i}  \\
                    \vdots  \\
                    e^{u_k}/\sum e^{u_i}  \\
                \end{bmatrix}
            \end{equation*}
            
            Where
            
            \begin{equation}
                u_i(x) = \theta_{(i)}^T x + \theta_{0(i)}
            \end{equation}
        \end{definition}
        
        
        If we are forced to make a \textbf{choice}, we choose the class with the \textbf{highest probability}: we return a \textbf{one-hot encoding}.

        \subsection*{A side comment: Sigmoid vs. Softmax}

            Let's pause real quick and clarify something.

            Usually, we expect to use \textbf{softmax} if we have more than 2 classes, because that's what we built it for.

            However, this isn't always the case. 
            
            There's another aspect we haven't focused on: \textbf{softmax} represents $k$ different classes/events. These classes are assumed to be \textbf{mutually exclusive}: you can't be in multiple at the same time.

            In other words, they are \textbf{disjoint}.\\

            \begin{definition}
                If two events are \vocab{disjoint}, they \purp{can't} happen at the \gren{same time}.

                If $n$ events are \vocab{disjoint}, only \purp{one} of them can happen at a time.
            \end{definition}

            \miniex We usually wouldn't classify an animal as both a cat and a dog: it's either one or the other.

            When events are disjoint, their probabilities are separate:\\

            \begin{concept}
                If two events are \vocab{disjoint}, then they have \gren{separate} probabilities: there's no overlap. Since $\p{A \cap B}=0$, we can say:

                \begin{equation*}
                    \p{A \cup B} = \p{A}+\p{B}
                \end{equation*}

                If we have \gren{every} event and they're all \vocab{disjoint}, then their probabilities sum to 1.

                \begin{equation}
                    \sum_i p_i = 1
                \end{equation}
            \end{concept}

            \miniex If the weather options are rain, cloudy, and sunny, and you have to only choose one, you should expect that:

            \begin{equation}
                \p{\text{Rain}} + \p{\text{Cloudy}} + \p{\text{Sunny}} = 1
            \end{equation}

            But this only makes sense \textbf{if} events can't happen at the same time.

            But, what if they can? For example: there might be $k$ different people we could find in an \textbf{image}. But, there can be \textbf{multiple} people in the same image! 

            So, it doesn't make sense to assume that each event is \textbf{mutually exclusive}: multiple events can all happen, which just isn't an option with softmax!

            The solution: we still have \textbf{probabilities}, so we just use \textbf{one sigmoid per class}.\\

            \begin{clarification}
                \purp{Softmax} is used when each of our $k$ classes is \purp{disjoint} (mutually exclusive).

                However, if they aren't, then we \gren{can't} use softmax.

                Instead, we use $k$ \purp{sigmoid} functions: one for each of our $k$ classes. We're using \vocab{binary classification} on each class separately.

                The $\nth{i}$ sigmoid tells us how likely the \purp{data point} is to be in the $\nth{i}$ class.
            \end{clarification}

            \miniex We might have an algorithm figuring out which \textbf{products} a customer might want. They might want \textbf{multiple}, so we can't treat them as disjoint.

            In this case, each product is a class, and we determine the result based on the matching sigmoid.

    