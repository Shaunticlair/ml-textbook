        \subsubsection{Weighted averages}
        
            We could naively average all of our gradients \textbf{equally}. But, this would be a bad idea:
            
            \begin{itemize}
                \item Gradients further in the past are \textbf{less likely} to matter. We also need to \textbf{scale} them down, so they don't take up most of the average.
                    \note{If you're averaging 100 terms, and you add one more... it's not going to change much.}
                    
                \item It doesn't give you as much control of the algorithm: what if we care more about the \textbf{present} gradient, than the previous one?
            \end{itemize}
            
            The second problem is easy to solve: we'll \textbf{weight} each of our terms differently.\\
            
            \begin{concept}
                Often, we want to factor in \gren{multiple} variables at the same time: we want to add or average them.
                
                But sometimes, some of those terms are \purp{more important} than others. We solve this by \vocab{weighing} those terms more heavily.
                
                We scale each term with a weight $w_i$: a larger $w_i$ magnitude means that term \gren{matters} more.
                
                Here, we introduce the \vocab{weighted sum}:
                
                \begin{equation*}
                    \text{Sum} = x_1w_1 + x_2w_2 + \dots + x_nw_n
                \end{equation*}
    
            \end{concept}
            
            \miniex Note that this looks almost the same as our neurons/regression models $w^Tx$. In both cases, the $w_i$ terms tell you how much each variable matters.
            
            But how do we turn this into an average? We need to \textbf{normalize} the weights.\\
            
            \begin{concept}
                We can think of each weight $w_i$ as the \purp{proportion} of the average that comes from $x_i$. For example, $w_1=\red{.5}$ means \red{50\%} of the average comes from $x_1$.
                
                If each term makes up a proportion of the average, it would make sense for their \gren{sum} to add up to \gren{1}.
                
                To make our weighted sum an \vocab{average}, we have to scale them so their sum add up to 1. \vocab{normalize} our weights.
                
                This is called \vocab{normalizing} our weights. We can do this two (equivalent) ways:
                
                \begin{itemize}
                    \item All of our weights add to \purp{one} before we average
                \end{itemize} 
                
                \begin{equation*}
                    \overbrace{
                        \sum_{i=1}^n w_i = 1
                    }^{\text{Weights add to 1}}
                \end{equation*}
                
                \begin{itemize}
                    \item Or, we \purp{divide} by their sum afterwards.
                \end{itemize}
                
                \begin{equation*}
                    \text{Average} = 
                    \overbrace{
                    \frac{1}{\sum_{i=1}^n w_i} 
                    }^{\text{Scale down weights}}
                    \overbrace{
                        \Bigg(
                        x_1w_1 + x_2w_2 + \dots + x_nw_n
                        \Bigg)
                    }^{\text{Weighted sum}}
                \end{equation*}
            \end{concept}
            
            Why is this necessary? Imagining you're averaging some number $x_1$ with itself: $x_2=x_1$. 
            
            \begin{equation}
                \text{Average} = x_1w_1+x_1w_2 = x_1(w_1+w_2)
            \end{equation}
            
            If our weights don't add up to 1, then we could average a number with itself, and get a different number! That's nonsense.
                \note{Imagine averaging ten copies of 3 and getting an average of "4". That makes no sense.}
            
            \subsecdiv
            
            
            
            
            \subsubsection{Running Average}
            
            This is a specific kind of \textbf{running} average:\\
        
            \begin{definition}
                A \vocab{running average} is a weighted average with \purp{weights}
                
                \begin{equation}
                    w =
                    \begin{bmatrix}
                        w_1 & w_2 & \cdots & w_m
                    \end{bmatrix}
                \end{equation}
                
                Applied to our \purp{data}
                
                \begin{equation*}
                        a 
                    = 
                    \begin{bmatrix}
                        a_0 & a_1 & \cdots & a_{n-1} & a_n
                    \end{bmatrix}
                \end{equation*}
                
                In a running average, we keep our weights, and \purp{shift} which data points we apply them to.
            \end{definition}
            
            \miniex One possible running average would be, "average the current data point, with the one before and the one after."
            
            Our weights would be
            
            \begin{equation}
                \begin{bmatrix}
                    1/3 & 1/3 & 1/3
                \end{bmatrix}
            \end{equation}
            
            If we had 
            
            
            
            
        \subsecdiv